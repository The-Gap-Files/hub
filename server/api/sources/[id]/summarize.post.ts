/**
 * POST /api/sources/[id]/summarize
 * 
 * Resume o conte√∫do de uma fonte do dossi√™ usando LangChain.
 * Para textos que excedem o contexto do modelo, usa estrat√©gia Map-Reduce:
 *   1. Divide o texto em chunks que cabem no contexto
 *   2. Resume cada chunk individualmente (MAP)
 *   3. Consolida os resumos parciais em um resumo final (REDUCE)
 */

import { z } from 'zod'
import { prisma } from '../../../utils/prisma'
import { SystemMessage, HumanMessage } from '@langchain/core/messages'
import { costLogService } from '../../../services/cost-log.service'
import { calculateLLMCost } from '../../../constants/pricing'
import { createLlmForTask, getAssignment } from '../../../services/llm/llm-factory'
import type { LlmTaskId } from '../../../constants/providers/llm-registry'

const BodySchema = z.object({
  save: z.boolean().default(false)
})

// ~80K tokens por chunk (320K chars), com margem para system prompt
const MAX_CHUNK_CHARS = 320_000

export default defineEventHandler(async (event) => {
  const sourceId = getRouterParam(event, 'id')

  if (!sourceId) {
    throw createError({
      statusCode: 400,
      message: 'Source ID is required'
    })
  }

  const source = await prisma.dossierSource.findUnique({
    where: { id: sourceId }
  })

  if (!source) {
    throw createError({
      statusCode: 404,
      message: 'Source not found'
    })
  }

  const wordCount = source.content.split(/\s+/).length
  if (wordCount < 500) {
    throw createError({
      statusCode: 422,
      message: `Conte√∫do j√° √© curto (${wordCount} palavras). Resumo n√£o necess√°rio.`
    })
  }

  const body = await readBody(event).catch(() => ({}))
  const { save } = BodySchema.parse(body)

  const TASK_ID: LlmTaskId = 'summarize'


  try {
    const llm = await createLlmForTask(TASK_ID, { temperature: 0.3, maxTokens: 8192 })
    const assignment = await getAssignment(TASK_ID)
    const startTime = Date.now()
    let totalInputTokens = 0
    let totalOutputTokens = 0

    // Decidir estrat√©gia: direto ou map-reduce
    const contentLength = source.content.length
    const needsChunking = contentLength > MAX_CHUNK_CHARS

    let summary: string

    if (!needsChunking) {
      // ‚ïê‚ïê‚ïê ESTRAT√âGIA DIRETA (cabe no contexto) ‚ïê‚ïê‚ïê
      console.log(`[SummarizeSource] üìù Resumindo fonte "${source.title}" (${wordCount} palavras) ‚Äî modo direto`)

      const result = await invokeSummarize(llm, source.content, source.title, source.sourceType)
      summary = result.summary
      totalInputTokens += result.inputTokens
      totalOutputTokens += result.outputTokens
    } else {
      // ‚ïê‚ïê‚ïê ESTRAT√âGIA MAP-REDUCE (excede o contexto) ‚ïê‚ïê‚ïê
      const chunks = splitIntoChunks(source.content, MAX_CHUNK_CHARS)
      console.log(`[SummarizeSource] üìù Resumindo fonte "${source.title}" (${wordCount} palavras) ‚Äî modo MAP-REDUCE (${chunks.length} chunks)`)

      // FASE MAP: resumir cada chunk
      const partialSummaries: string[] = []
      let chunkIndex = 0
      for (const chunk of chunks) {
        const chunkWords = chunk.split(/\s+/).length
        console.log(`[SummarizeSource]   üìÑ Chunk ${chunkIndex + 1}/${chunks.length} (~${chunkWords.toLocaleString()} palavras)...`)

        const result = await invokeSummarize(
          llm,
          chunk,
          `${source.title} (Parte ${chunkIndex + 1}/${chunks.length})`,
          source.sourceType,
          true // isChunk
        )
        partialSummaries.push(result.summary)
        totalInputTokens += result.inputTokens
        totalOutputTokens += result.outputTokens
        chunkIndex++
      }

      // FASE REDUCE: consolidar os resumos parciais
      console.log(`[SummarizeSource]    Consolidando ${partialSummaries.length} resumos parciais...`)
      const combinedText = partialSummaries.map((s, i) => `=== PARTE ${i + 1} ===\n${s}`).join('\n\n')

      const reduceResult = await invokeReduce(llm, combinedText, source.title)
      summary = reduceResult.summary
      totalInputTokens += reduceResult.inputTokens
      totalOutputTokens += reduceResult.outputTokens
    }

    const elapsed = ((Date.now() - startTime) / 1000).toFixed(2)
    const summaryWordCount = summary.split(/\s+/).length

    console.log(`[SummarizeSource] ‚úÖ Resumo: ${wordCount.toLocaleString()} ‚Üí ${summaryWordCount.toLocaleString()} palavras (${elapsed}s)`)
    console.log(`[SummarizeSource] üìä Tokens totais: ${totalInputTokens.toLocaleString()} in + ${totalOutputTokens.toLocaleString()} out`)

    if (save) {
      await prisma.dossierSource.update({
        where: { id: sourceId },
        data: { content: summary }
      })
      console.log(`[SummarizeSource] üíæ Resumo salvo na fonte ${sourceId}`)
    }

    // Registrar custo
    const modelUsed = assignment.model
    const summaryCost = calculateLLMCost(modelUsed, totalInputTokens, totalOutputTokens)

    costLogService.log({
      dossierId: source.dossierId,
      resource: 'insights',
      action: 'create',
      provider: assignment.provider.toUpperCase(),
      model: modelUsed,
      cost: summaryCost,
      metadata: { input_tokens: totalInputTokens, output_tokens: totalOutputTokens, total_tokens: totalInputTokens + totalOutputTokens },
      detail: `Resumo de fonte: ${source.title}${needsChunking ? ' (map-reduce)' : ''}`
    }).catch((err: any) => console.error('[SummarizeSource] CostLog:', err))

    return {
      success: true,
      summary,
      originalWordCount: wordCount,
      summaryWordCount,
      saved: save,
      chunked: needsChunking,
      usage: { inputTokens: totalInputTokens, outputTokens: totalOutputTokens }
    }
  } catch (error: any) {
    console.error('[SummarizeSource] ‚ùå Erro:', error)
    throw createError({
      statusCode: 500,
      message: error.message || 'Erro ao resumir conte√∫do'
    })
  }
})

// =============================================================================
// HELPERS
// =============================================================================

/** Divide texto em chunks respeitando quebras de par√°grafos */
function splitIntoChunks(text: string, maxChars: number): string[] {
  if (text.length <= maxChars) return [text]

  const chunks: string[] = []
  let remaining = text

  while (remaining.length > 0) {
    if (remaining.length <= maxChars) {
      chunks.push(remaining)
      break
    }

    // Tentar cortar em quebra de par√°grafo (\n\n)
    let cutPoint = remaining.lastIndexOf('\n\n', maxChars)

    // Se n√£o encontrou par√°grafo, tentar quebra de linha
    if (cutPoint < maxChars * 0.5) {
      cutPoint = remaining.lastIndexOf('\n', maxChars)
    }

    // Se ainda n√£o encontrou, cortar em espa√ßo
    if (cutPoint < maxChars * 0.5) {
      cutPoint = remaining.lastIndexOf(' ', maxChars)
    }

    // √öltimo recurso: cortar no limite
    if (cutPoint < maxChars * 0.3) {
      cutPoint = maxChars
    }

    chunks.push(remaining.slice(0, cutPoint))
    remaining = remaining.slice(cutPoint).trimStart()
  }

  return chunks
}

/** Fase MAP: resume um chunk individual */
async function invokeSummarize(
  llm: any,
  content: string,
  title: string,
  sourceType: string,
  isChunk: boolean = false
): Promise<{ summary: string; inputTokens: number; outputTokens: number }> {
  const systemPrompt = `Voc√™ √© um especialista em s√≠ntese de informa√ß√µes para produ√ß√£o de conte√∫do audiovisual.

Sua tarefa √© resumir o texto fornecido, mantendo TODAS as informa√ß√µes essenciais:
- Fatos, datas, nomes e locais espec√≠ficos
- Dados estat√≠sticos e n√∫meros
- Cita√ß√µes relevantes
- Rela√ß√µes causais e cronol√≥gicas
- Detalhes que gerem engajamento narrativo

REGRAS:
${isChunk
      ? '- Mantenha o resumo entre 1000 e 2000 palavras (este √© um trecho parcial do documento)'
      : '- Mantenha o resumo entre 1500 e 2500 palavras'}
- Preserve dados factuais ‚Äî NUNCA invente informa√ß√µes
- Escreva em portugu√™s brasileiro
- Use par√°grafos curtos e objetivos
- Mantenha a estrutura l√≥gica do material original
- Priorize informa√ß√µes √∫nicas e diferenciadas`

  const userPrompt = `Resuma o seguinte texto de forma concisa mas completa:\n\n---\nT√çTULO: ${title}\nTIPO: ${sourceType}\n---\n\n${content}`

  const result = await llm.invoke([
    new SystemMessage(systemPrompt),
    new HumanMessage(userPrompt)
  ])

  const usage = result.usage_metadata || result.response_metadata?.usage
  return {
    summary: result.content as string,
    inputTokens: usage?.input_tokens ?? 0,
    outputTokens: usage?.output_tokens ?? 0
  }
}

/** Fase REDUCE: consolida resumos parciais em um resumo final */
async function invokeReduce(
  llm: any,
  combinedSummaries: string,
  title: string
): Promise<{ summary: string; inputTokens: number; outputTokens: number }> {
  const systemPrompt = `Voc√™ √© um especialista em s√≠ntese editorial. Sua tarefa √© consolidar m√∫ltiplos resumos parciais de um mesmo documento em um resumo final √∫nico, coeso e completo.

REGRAS:
- Mantenha o resumo final entre 2000 e 3000 palavras
- Elimine redund√¢ncias entre as partes
- Mantenha a estrutura cronol√≥gica/l√≥gica do material
- Preserve TODOS os dados factuais: datas, nomes, n√∫meros e estat√≠sticas
- Escreva em portugu√™s brasileiro
- Produza um texto fluido, como se fosse um resumo √∫nico desde o in√≠cio
- NUNCA invente informa√ß√µes que n√£o estejam nos resumos parciais`

  const userPrompt = `Consolide os seguintes resumos parciais do documento "${title}" em um resumo final √∫nico e coeso:\n\n${combinedSummaries}`

  const result = await llm.invoke([
    new SystemMessage(systemPrompt),
    new HumanMessage(userPrompt)
  ])

  const usage = result.usage_metadata || result.response_metadata?.usage
  return {
    summary: result.content as string,
    inputTokens: usage?.input_tokens ?? 0,
    outputTokens: usage?.output_tokens ?? 0
  }
}
