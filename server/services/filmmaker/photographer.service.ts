/**
 * Photographer Service (Agente FotÃ³grafo)
 *
 * Primeiro agente do pipeline de 3:
 * FotÃ³grafo â†’ CoreÃ³grafo â†’ Cineasta
 *
 * Gera `visualDescription` (start image prompt) com qualidade fotogrÃ¡fica:
 * lente, DOF, iluminaÃ§Ã£o, texturas, coerÃªncia temporal, progressÃ£o narrativa.
 */

import { createLlmForTask } from '../llm/llm-factory'
import { HumanMessage, SystemMessage } from '@langchain/core/messages'
import { createPipelineLogger } from '../../utils/pipeline-logger'
import type { SceneInput, ProductionContext, PhotographerOutput } from './filmmaker.types'
import type { StoryOutline } from '../story-architect.service'
import fs from 'node:fs/promises'
import path from 'node:path'

const LOG_STAGE = 'Photographer'

export class PhotographerService {

  private async loadSkill(): Promise<string> {
    const skillPath = path.resolve(process.cwd(), 'server/skills/photographer.md')
    try {
      return await fs.readFile(skillPath, 'utf-8')
    } catch {
      console.warn(`[${LOG_STAGE}] âš ï¸ photographer.md nÃ£o encontrado, usando fallback.`)
      return 'You are an expert still photographer for cinematic AI image generation. Generate rich visualDescription prompts (50-120 words) with lens, DOF, lighting, textures.'
    }
  }

  /**
   * Extrai pistas temporais (dÃ©cada, ano, perÃ­odo) do tema do dossiÃª.
   */
  private extractTemporalHint(theme: string): string | undefined {
    const patterns = [
      /anos\s*(\d{4})/i,
      /dÃ©cada\s*de\s*(\d{2,4})/i,
      /(\d{4})\s*[-â€“a]\s*(\d{4})/,
      /(\d{4})s/,
      /in\s+the\s+(\d{4})s/i,
      /(?:year|ano)\s+(\d{4})/i,
      /sÃ©culo\s+(X{0,3}(?:IX|IV|V?I{0,3}))/i,
      /(\d{4})/,
    ]
    for (const pattern of patterns) {
      const match = theme.match(pattern)
      if (match) {
        const raw = match[0]
        const yearMatch = raw.match(/(\d{4})/)
        if (yearMatch) {
          const year = parseInt(yearMatch[1]!)
          if (year >= 1800 && year <= 2030) {
            const decade = Math.floor(year / 10) * 10
            return `dÃ©cada de ${decade} (${raw})`
          }
        }
        const shortDecade = raw.match(/(\d{2})/)
        if (shortDecade) {
          const dec = parseInt(shortDecade[1]!)
          if (dec >= 50 && dec <= 99) return `dÃ©cada de 19${dec} (${raw})`
          if (dec >= 0 && dec <= 30) return `dÃ©cada de 20${String(dec).padStart(2, '0')} (${raw})`
        }
        return raw
      }
    }
    return undefined
  }

  /**
   * Monta Production Awareness para o system prompt do FotÃ³grafo.
   * Inclui: contexto temporal, style anchor, visual identity, narrative awareness.
   */
  private buildProductionAwareness(production?: ProductionContext): string {
    if (!production) return ''

    const sections: string[] = []

    if (production.theme) {
      const temporalHint = this.extractTemporalHint(production.theme)
      sections.push(`[TEMPORAL CONTEXT â€” Ã©poca e cenÃ¡rio do assunto]
Tema: "${production.theme}"${temporalHint ? `\nPerÃ­odo detectado: ${temporalHint}` : ''}
â†’ TODOS os elementos visuais DEVEM ser period-accurate para esta Ã©poca/contexto.
â†’ Checklist: veÃ­culos, tecnologia, vestuÃ¡rio, arquitetura, sinalizaÃ§Ã£o, mobiliÃ¡rio, iluminaÃ§Ã£o.
â†’ Se o tema menciona uma dÃ©cada/ano, NENHUM elemento visual pode ser de Ã©poca posterior.`)
    }

    if (production.styleAnchorTags) {
      sections.push(`[STYLE ANCHOR â€” jÃ¡ aplicado pelo pipeline ao prompt final de imagem]
"${production.styleAnchorTags}"
â†’ NÃƒO repita essas tags no visualDescription. Elas jÃ¡ serÃ£o injetadas como prefixo.
â†’ Foque em parÃ¢metros COMPLEMENTARES: lente, distÃ¢ncia focal, DOF, origem da luz, texturas.`)
    }

    if (production.visualIdentity) {
      sections.push(`[VISUAL IDENTITY â€” diretrizes do universo/dossiÃª]
"${production.visualIdentity}"
â†’ Incorpore organicamente nas descriÃ§Ãµes visuais (perÃ­odo, materialidade, paleta).`)
    }

    const narrativeBlock = production.storyOutline
      ? this.buildNarrativeAwareness(production.storyOutline)
      : ''

    if (sections.length === 0 && !narrativeBlock) return ''

    return `\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PRODUCTION AWARENESS (contexto do pipeline):
${sections.join('\n\n')}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€${narrativeBlock}`
  }

  private buildNarrativeAwareness(outline: StoryOutline): string {
    const lines: string[] = [
      ``,
      `â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`,
      `NARRATIVE AWARENESS:`,
      `[ARCO EMOCIONAL]    : ${outline.emotionalArc || 'NÃ£o definido'}`,
      `[PROGRESSÃƒO DE TOM] : ${outline.toneProgression || 'NÃ£o definido'}`,
      `[FÃ“RMULA DO CLÃMAX] : ${outline.climaxFormula || 'NÃ£o definido'}`,
      `[MOMENTO DE CLÃMAX] : ${(outline.climaxMoment || 'RevelaÃ§Ã£o central').slice(0, 100)}`,
      `[RESOLUÃ‡ÃƒO]         : ${outline.resolutionLevel || 'full'}`,
      ``,
      `Guia de Intensidade Visual por Segmento:`,
      `  HOOK       â†’ Alta intensidade. Ruptura visual imediata.`,
      `  CONTEXT    â†’ Baixa-mÃ©dia. Planos abertos, luz natural.`,
      `  RISING     â†’ ProgressÃ£o. Siga a tensÃ£o cena a cena.`,
      `  CLIMAX     â†’ PICO ABSOLUTO. MÃ¡ximo contraste.`,
      `  RESOLUTION â†’ ReduÃ§Ã£o gradual. Aterramento emocional.`,
      `  CTA        â†’ MÃ­nima. Limpa. NÃ£o distrai.`,
    ]

    if (outline.tensionCurve && outline.tensionCurve.length > 0) {
      lines.push(``)
      lines.push(`Tension Curve (seÃ§Ã£o RISING):`)
      lines.push(outline.tensionCurve.map((level, i) => `  Beat ${i + 1}: ${level.toUpperCase()}`).join('\n'))
    }

    lines.push(`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`)
    return lines.join('\n')
  }

  /**
   * AnotaÃ§Ãµes narrativas por cena (segmento, tensÃ£o, contexto do beat).
   */
  private buildSceneNarrativeAnnotations(
    outline: StoryOutline,
    totalScenes: number
  ): Array<{ segment: string; tensionLevel: string; note: string }> {
    const dist = outline.segmentDistribution
    if (!dist) return []

    const segments = [
      { name: 'HOOK', count: dist.hook, defaultTension: 'high' },
      { name: 'CONTEXT', count: dist.context, defaultTension: 'low' },
      { name: 'RISING', count: dist.rising, defaultTension: 'medium' },
      { name: 'CLIMAX', count: dist.climax, defaultTension: 'peak' },
      { name: 'RESOLUTION', count: dist.resolution, defaultTension: 'medium' },
      { name: 'CTA', count: dist.cta, defaultTension: 'low' }
    ]

    const annotations: Array<{ segment: string; tensionLevel: string; note: string }> = []
    const tensionCurve = outline.tensionCurve || []
    const risingBeats = outline.risingBeats || []
    const risingCount = dist.rising

    for (const seg of segments) {
      for (let i = 0; i < seg.count; i++) {
        if (annotations.length >= totalScenes) break
        let tension = seg.defaultTension
        let note = seg.name

        if (seg.name === 'HOOK') {
          note = `HOOK â€” ${(outline.hookStrategy || 'Opening impact').slice(0, 80)}`
        } else if (seg.name === 'RISING' && risingCount > 0) {
          const beatIdx = tensionCurve.length > 0
            ? Math.min(Math.floor((i / risingCount) * tensionCurve.length), tensionCurve.length - 1)
            : -1
          if (beatIdx >= 0 && tensionCurve[beatIdx]) tension = tensionCurve[beatIdx]
          const beat = risingBeats[Math.min(beatIdx >= 0 ? beatIdx : 0, risingBeats.length - 1)]
          note = beat
            ? `RISING beat ${beat.order}: "${beat.revelation.slice(0, 70)}"`
            : `RISING â€” scene ${i + 1}/${risingCount}`
        } else if (seg.name === 'CLIMAX') {
          note = `CLIMAX (${outline.climaxFormula || 'peak'}) â€” ${(outline.climaxMoment || 'Central revelation').slice(0, 80)}`
        } else if (seg.name === 'RESOLUTION') {
          const rl = outline.resolutionLevel
          note = `RESOLUTION â€” ${rl === 'none' ? 'ZERO resolution' : rl === 'partial' ? 'Partial resolution' : 'Full resolution'}`
        } else if (seg.name === 'CTA') {
          note = `CTA â€” ${(outline.ctaApproach || 'Closing').slice(0, 60)}`
        }
        annotations.push({ segment: seg.name, tensionLevel: tension, note })
      }
    }

    while (annotations.length < totalScenes) {
      annotations.push({ segment: 'EXTRA', tensionLevel: 'low', note: 'Overflow scene â€” treat as CTA/closing' })
    }

    return annotations
  }

  /**
   * Monta continuidade visual (cenas que compartilham ambiente).
   */
  private buildContinuityContext(scenes: SceneInput[]): string {
    const envGroups = new Map<string, number[]>()
    scenes.forEach((s, i) => {
      if (s.currentEnvironment) {
        const group = envGroups.get(s.currentEnvironment) || []
        group.push(i)
        envGroups.set(s.currentEnvironment, group)
      }
    })

    const sharedEnvs = [...envGroups.entries()].filter(([, indices]) => indices.length > 1)
    if (sharedEnvs.length === 0) return ''

    const envList = sharedEnvs.map(([env, indices]) =>
      `- "${env}": cenas ${indices.join(', ')}`
    ).join('\n')

    return `
CONTINUIDADE VISUAL (cenas no mesmo ambiente):
${envList}
â†’ Mesmo ambiente = mesma lente, temperatura de cor, materiais, origem de luz.
â†’ Varie apenas: Ã¢ngulo, enquadramento, foreground, elementos dinÃ¢micos.`
  }

  /**
   * Refina as cenas gerando visualDescription via LLM.
   */
  async refine(
    scenes: SceneInput[],
    baseStyle: string,
    production?: ProductionContext
  ): Promise<PhotographerOutput[]> {
    const log = createPipelineLogger({ stage: LOG_STAGE, outputId: 'photographer' })

    const skillContent = await this.loadSkill()
    const productionAwareness = this.buildProductionAwareness(production)

    const systemPrompt = `${skillContent}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ESTILO VISUAL BASE (USE COMO PRIMEIRO ELEMENTO):
"${baseStyle}"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
${productionAwareness}`

    const narrativeAnnotations = production?.storyOutline
      ? this.buildSceneNarrativeAnnotations(production.storyOutline, scenes.length)
      : null

    const continuityContext = this.buildContinuityContext(scenes)

    const userPrompt = `CENAS DO ROTEIRO PARA FOTOGRAFAR:

${JSON.stringify(
  scenes.map((s, i) => {
    const ann = narrativeAnnotations?.[i]
    return {
      order: i,
      ...(ann ? {
        narrativeSegment: ann.segment,
        tensionLevel: ann.tensionLevel,
        narrativeNote: ann.note
      } : {}),
      narration: s.narration,
      environment: s.currentEnvironment || null,
      durationSeconds: s.estimatedDuration
    }
  }),
  null,
  2
)}
${continuityContext}
${production?.customSceneReferences && production.customSceneReferences.length > 0
  ? `\nðŸŽ¬ IMAGENS DE REFERÃŠNCIA DO CRIADOR:
${production.customSceneReferences.map(ref => {
  const promptNote = ref.imagePrompt ? ` | Prompt original: "${ref.imagePrompt}"` : ''
  return `- Cena ${ref.sceneOrder}: "${ref.description}"${promptNote} â†’ Seu visualDescription DEVE incorporar elementos desta referÃªncia.`
}).join('\n')}`
  : ''}

TAREFA:
Para CADA cena, escreva o visualDescription â€” o prompt completo para gerar a imagem INICIAL da cena (em inglÃªs).

Retorne APENAS um JSON vÃ¡lido (sem markdown, sem explicaÃ§Ãµes):
{
  "scenes": [
    { "order": 0, "visualDescription": "...", "sceneEnvironment": "..." }
  ]
}`

    log.info(`ðŸ“¸ Chamando LLM para fotografar ${scenes.length} cenas...`)
    const llm = await createLlmForTask('photographer', { temperature: 0.7, maxTokens: 40000 })

    const response = await llm.invoke([
      new SystemMessage(systemPrompt),
      new HumanMessage(userPrompt)
    ])

    const rawText = typeof response.content === 'string'
      ? response.content
      : JSON.stringify(response.content)

    try {
      const cleanJson = rawText.replace(/```json\n?|```/g, '').trim()
      const parsed = JSON.parse(cleanJson) as { scenes: PhotographerOutput[] }

      if (!Array.isArray(parsed.scenes)) {
        throw new Error('Formato invÃ¡lido: "scenes" nÃ£o Ã© array.')
      }

      log.info(`âœ… FotÃ³grafo retornou ${parsed.scenes.length} cenas.`)
      return parsed.scenes
    } catch (error) {
      console.error(`[${LOG_STAGE}] Falha ao parsear JSON:`, error)
      console.error(`[${LOG_STAGE}] Raw (500 chars):`, rawText.slice(0, 500))

      // Fallback: retorna cenas originais sem alteraÃ§Ã£o
      return scenes.map((s, i) => ({
        order: i,
        visualDescription: s.currentVisual || '',
        sceneEnvironment: s.currentEnvironment
      }))
    }
  }
}

export const photographer = new PhotographerService()
