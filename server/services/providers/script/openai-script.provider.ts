import { z } from 'zod'
import { ChatOpenAI } from '@langchain/openai'
import {
  SystemMessage,
  HumanMessage,
  BaseMessage
} from '@langchain/core/messages'
import type {
  IScriptGenerator,
  ProviderCostInfo,
  ScriptGenerationRequest,
  ScriptGenerationResponse,
  ScriptScene
} from '../../../types/ai-providers'
import { calculateLLMCost } from '../../../constants/pricing'
import { buildVisualInstructionsForScript } from '../../../utils/wan-prompt-builder'
import { formatPersonsForPrompt, formatNeuralInsightsForPrompt } from '../../../utils/format-intelligence-context'

// Schema para valida√ß√£o estruturada do output (Garante JSON v√°lido e tipos corretos)
const ScriptSceneSchema = z.object({
  order: z.number().describe('A ordem sequencial da cena'),
  narration: z.string().describe('O texto que ser√° narrado pelo locutor'),
  visualDescription: z.string().describe('Descri√ß√£o t√©cnica e sensorial para o modelo de gera√ß√£o de v√≠deo (SEMPRE EM INGL√äS)'),
  audioDescription: z.string().nullable().describe('Atmosfera sonora e SFX em ingl√™s t√©cnico'),
  estimatedDuration: z.number().describe('Dura√ß√£o estimada em segundos (entre 5 e 6 segundos)')
})

const BackgroundMusicTrackSchema = z.object({
  prompt: z.string().describe('Prompt para gera√ß√£o de m√∫sica no formato Stable Audio. Inclua g√™nero, instrumentos, BPM, mood e estilo. Exemplo: "Ambient, Drone, Dark Strings, Pulsing Heartbeat Rhythm, Tension Build-Up, Mysterious, Cinematic, Atmospheric, 80 BPM"'),
  volume: z.number().describe('Volume em dB para mixagem com narra√ß√£o (-24 a -6). Prefira -12 a -10 para fundo claramente aud√≠vel; -18 fica baixo demais. Ex.: -12 m√©dio, -10 mais presente, -6 alto.'),
  startScene: z.number().describe('N√∫mero da cena onde esta track come√ßa (0 = primeira cena)'),
  endScene: z.number().nullable().describe('N√∫mero da √∫ltima cena desta track (null = at√© a √∫ltima cena do v√≠deo)')
})

const BackgroundMusicSchema = z.object({
  prompt: z.string().describe('Prompt para gera√ß√£o de m√∫sica no formato Stable Audio. Inclua g√™nero, instrumentos, BPM, mood e estilo. Exemplo: "Ambient, Drone, Dark Strings, Subtle Pads, Mysterious, Cinematic, Atmospheric, well-arranged composition, 80 BPM"'),
  volume: z.number().describe('Volume em dB para mixagem com narra√ß√£o (-24 a -6). Prefira -12 a -10 para fundo claramente aud√≠vel; -18 fica baixo demais. Ex.: -12 m√©dio, -10 mais presente, -6 alto.')
})

const ScriptResponseSchema = z.object({
  title: z.string().describe('T√≠tulo impactante para o v√≠deo'),
  summary: z.string().describe('Sinopse intrigante de 2-3 par√°grafos'),
  scenes: z.array(ScriptSceneSchema).describe('Lista de cenas que comp√µem o v√≠deo'),
  backgroundMusic: BackgroundMusicSchema.nullable().describe('M√∫sica de fundo √∫nica para TODO o v√≠deo (use apenas para v√≠deos curtos TikTok/Instagram). Use null para v√≠deos longos. Regra: "video todo"'),
  backgroundMusicTracks: z.array(BackgroundMusicTrackSchema).nullable().describe('Lista de tracks de m√∫sica de fundo por segmento de cenas (use apenas para v√≠deos longos YouTube Cinematic). Use null para v√≠deos curtos. Cada track define uma m√∫sica com prompt, volume, startScene e endScene.')
})

type ScriptResponse = z.infer<typeof ScriptResponseSchema>

export class OpenAIScriptProvider implements IScriptGenerator {
  private model: ChatOpenAI
  private modelName: string

  constructor(config: { apiKey: string; model?: string; baseUrl?: string }) {
    this.modelName = config.model ?? 'gpt-4o'
    this.model = new ChatOpenAI({
      openAIApiKey: config.apiKey,
      modelName: this.modelName,
      configuration: {
        baseURL: config.baseUrl ?? 'https://api.openai.com/v1'
      },
      temperature: 0.7,
      timeout: 120000, // 2 minutos de timeout para chamadas multimodais complexas
      maxRetries: 2
    })
  }

  getName(): string {
    return 'OPENAI'
  }

  async generate(request: ScriptGenerationRequest): Promise<ScriptGenerationResponse> {
    console.log(`[OpenAI Script] üé¨ Iniciando gera√ß√£o de roteiro via LangChain (${this.modelName})...`)

    // Configurar o modelo para output estruturado (Zod) com includeRaw para capturar token usage
    const structuredLlm = this.model.withStructuredOutput(ScriptResponseSchema, { includeRaw: true })

    const systemPrompt = this.buildSystemPrompt(request)
    const userPrompt = this.buildUserPrompt(request)

    // Log para depura√ß√£o
    console.log('--- [DEBUG] LANGCHAIN CONFIGURATION ---')
    console.log('Target Duration:', request.targetDuration, 'seconds')
    console.log('Target WPM:', request.targetWPM)
    console.log('Ideal Scene Count:', Math.ceil(request.targetDuration / 5))
    console.log('--- [DEBUG] LANGCHAIN SYSTEM PROMPT ---')
    console.log(systemPrompt)

    // Preparar mensagens (Suporte Multimodal)
    const messages: BaseMessage[] = [
      new SystemMessage(systemPrompt)
    ]

    // Construir conte√∫do da mensagem do usu√°rio (Texto + Imagens)
    const humanContent: any[] = [
      { type: 'text', text: userPrompt }
    ]

    // Injetar imagens se dispon√≠veis (Vision Capability)
    if (request.images && request.images.length > 0) {
      console.log(`[OpenAI Script] üëÅÔ∏è Injetando ${request.images.length} imagens no contexto multimodal...`)

      request.images.forEach((img, idx) => {
        let base64Data = ''

        try {
          if (Buffer.isBuffer(img.data)) {
            base64Data = img.data.toString('base64')
          } else if (typeof img.data === 'string') {
            base64Data = img.data
          } else if (typeof img.data === 'object') {
            // Tenta tratar como Buffer-like ou Uint8Array
            // Prisma pode retornar Uint8Array que n√£o √© intelisense Buffer
            // JSON serializado { type: 'Buffer', data: [...] } tamb√©m cai aqui se passado no Buffer.from
            // JSON serializado { type: 'Buffer', data: [...] } tamb√©m cai aqui se passado no Buffer.from
            if ((img.data as any).type === 'Buffer' && Array.isArray((img.data as any).data)) {
              base64Data = Buffer.from((img.data as any).data).toString('base64')
            } else {
              // Fallback gen√©rico para Uint8Array ou Array-like object
              base64Data = Buffer.from(img.data as any).toString('base64')
            }
          }
        } catch (e) {
          console.warn(`[OpenAI Script] ‚ö†Ô∏è Falha ao converter imagem ${idx}. Erro: ${e}`)
        }

        if (!base64Data) {
          console.warn(`[OpenAI Script] ‚ö†Ô∏è Imagem ${idx} ignorada: falha na extra√ß√£o de dados. Tipo: ${typeof img.data}`)
          if (typeof img.data === 'object') {
            try {
              const preview = JSON.stringify(img.data).slice(0, 100)
              console.warn(`[OpenAI Script] üîç Preview do objeto de dados: ${preview}...`)
            } catch { }
          }
          return
        }

        // Remover prefixo data:image/...;base64, se j√° existir na string para evitar duplica√ß√£o
        if (base64Data.includes('base64,')) {
          base64Data = base64Data.split('base64,')[1] || ''
        }

        const validMimeTypes = ['image/jpeg', 'image/png', 'image/gif', 'image/webp']
        let mimeType = (img.mimeType || 'image/jpeg').toLowerCase()

        // Normalizar tipos comuns malformados
        if (mimeType === 'image/jpg') mimeType = 'image/jpeg'

        if (!validMimeTypes.includes(mimeType)) {
          console.warn(`[OpenAI Script] ‚ö†Ô∏è Imagem ${idx} ignorada: formato n√£o suportado (${mimeType}). Permitidos: ${validMimeTypes.join(', ')}`)
          return
        }

        // console.log(`[OpenAI Script] Processando imagem ${idx}: ${mimeType}, Base64 Length: ${base64Data.length}`)

        humanContent.push({
          type: 'image_url',
          image_url: {
            url: `data:${mimeType};base64,${base64Data}`,
            detail: 'high'
          }
        })
      })
    }

    messages.push(new HumanMessage({ content: humanContent }))

    try {
      const startTime = Date.now()
      console.log('[OpenAI Script] üì§ Enviando request multimodal para LangChain...')
      console.log('[OpenAI Script] üîç Schema esperado: title, summary, scenes, backgroundMusic, backgroundMusicTracks')

      const result = await structuredLlm.invoke(messages)
      const content = result.parsed as ScriptResponse
      const rawMessage = result.raw as any

      const elapsed = ((Date.now() - startTime) / 1000).toFixed(2)
      console.log(`[OpenAI Script] üì• Resposta recebida e validada em ${elapsed}s`)

      // Extrair token usage real da resposta
      const usage = rawMessage?.usage_metadata || rawMessage?.response_metadata?.usage
      const inputTokens = usage?.input_tokens ?? 0
      const outputTokens = usage?.output_tokens ?? 0
      const totalTokens = usage?.total_tokens ?? (inputTokens + outputTokens)

      console.log(`[OpenAI Script] üìä Token Usage REAL: ${inputTokens} input + ${outputTokens} output = ${totalTokens} total`)

      // Valida√ß√£o r√°pida de integridade
      console.log('[OpenAI Script] ‚úÖ Roteiro gerado com sucesso!')
      console.log('[OpenAI Script] T√≠tulo:', content.title)
      console.log('[OpenAI Script] N√∫mero de cenas:', content.scenes.length)
      console.log('[OpenAI Script] Background Music:', content.backgroundMusic ? 'Sim (video todo)' : 'N√£o')
      console.log('[OpenAI Script] Background Music Tracks:', content.backgroundMusicTracks?.length || 0, 'tracks')

      return this.parseResponse(content, request, { inputTokens, outputTokens, totalTokens })
    } catch (error) {
      console.error('[OpenAI Script] ‚ùå Erro na gera√ß√£o estruturada:', error)
      console.error('[OpenAI Script] üîç Error details:', JSON.stringify(error, null, 2))
      throw error
    }
  }

  private buildSystemPrompt(request: ScriptGenerationRequest): string {
    let styleInstructions = request.scriptStyleInstructions || 'Adote um tom documental s√©rio e investigativo.'

    let visualInstructions = ''
    if (request.visualBaseStyle) {
      visualInstructions = buildVisualInstructionsForScript({
        baseStyle: request.visualBaseStyle,
        lightingTags: request.visualLightingTags || '',
        atmosphereTags: request.visualAtmosphereTags || '',
        compositionTags: request.visualCompositionTags || '',
        generalTags: request.visualGeneralTags
      })
    } else if (request.visualStyleDescription) {
      visualInstructions = `DIRETRIZ VISUAL OBRIGAT√ìRIA: ${request.visualStyleDescription}`
    } else {
      visualInstructions = buildVisualInstructionsForScript({
        baseStyle: 'Cinematic Mystery Documentary',
        lightingTags: 'Chiaroscuro, dramatic volumetric lighting, shadows dancing',
        atmosphereTags: 'Mysterious, moody, foggy, dense atmosphere',
        compositionTags: 'Cinematic wide shots, extreme close-ups on textures',
        generalTags: '4k, highly detailed, realistic textures, grainy film look'
      })
    }

    const targetWPM = request.targetWPM || 150
    const wordsPerScene = Math.round((targetWPM / 60) * 5) // 150 WPM = 12-13 palavras por 5s
    const maxWordsHard = wordsPerScene + 2 // Hard limit: nunca exceder (15 palavras a 150 WPM)
    const wordRange = `${wordsPerScene - 1}-${wordsPerScene + 1}`

    // Determinar formato do v√≠deo para instru√ß√µes de m√∫sica
    const videoFormat = request.format || request.outputType || 'full-youtube'
    const isShortFormat = videoFormat.includes('tiktok') || videoFormat.includes('reels') || videoFormat.includes('teaser')
    const isYouTubeCinematic = videoFormat.includes('youtube') || videoFormat.includes('full')

    let musicInstructions = ''
    if (isShortFormat) {
      musicInstructions = `
---
üéµ ESTRAT√âGIA DE M√öSICA DE FUNDO (TikTok/Instagram):
- üö® REGRA: "video todo" - Use UMA m√∫sica de fundo para TODO o v√≠deo do in√≠cio ao fim
- Use o campo "backgroundMusic" com "prompt" e "volume"
- O "prompt" ser√° usado diretamente no modelo Stable Audio 2.5 para gerar a m√∫sica
- FORMATO DO PROMPT: Inclua g√™nero, sub-g√™nero, instrumentos espec√≠ficos, BPM, mood e estilo
- O "volume" √© em dB para mixagem com narra√ß√£o (-24 a -6). Prefira -12 a -10 para a m√∫sica ser claramente aud√≠vel; -18 costuma ficar baixo demais.
- Exemplo de prompt: "Ambient, Dark Drone, Subtle Synthesizer Pads, Low Strings, Mysterious, Cinematic, Atmospheric, well-arranged composition, 80 BPM"
- N√ÉO inclua volume no prompt - o volume √© um campo separado
- Exemplo completo: { prompt: "Ambient, Dark Drone, Subtle Pads, Mysterious, Cinematic, 80 BPM", volume: -12 }`
    } else if (isYouTubeCinematic) {
      musicInstructions = `
---
üéµ ESTRAT√âGIA DE M√öSICA DE FUNDO (YouTube Cinematic):
- Use a lista "backgroundMusicTracks" para definir tracks por SEGMENTO DE CENAS
- Cada track tem: "prompt" (para Stable Audio 2.5), "volume" (dB), "startScene" e "endScene"
- "startScene" = n√∫mero da cena onde a track come√ßa (0 = primeira cena)
- "endScene" = n√∫mero da √∫ltima cena desta track (null = at√© a √∫ltima cena do v√≠deo)
- O "prompt" ser√° usado diretamente no modelo Stable Audio 2.5 para gerar cada track
- FORMATO DO PROMPT: Inclua g√™nero, sub-g√™nero, instrumentos espec√≠ficos, BPM, mood e estilo
- O "volume" √© em dB para mixagem com narra√ß√£o (-24 a -6). Prefira -12 a -10 para a m√∫sica ser claramente aud√≠vel; -18 costuma ficar baixo demais.
- N√ÉO fa√ßa uma track por cena. Agrupe cenas por SEGMENTOS NARRATIVOS:
  ‚Ä¢ HOOK: Cenas iniciais ‚Äî m√∫sica de abertura impactante
  ‚Ä¢ CONTEXT: Cenas de contextualiza√ß√£o ‚Äî transi√ß√£o suave
  ‚Ä¢ RISING ACTION: Corpo principal ‚Äî intensidade crescente
  ‚Ä¢ CLIMAX: Pico narrativo ‚Äî m√°xima intensidade emocional
  ‚Ä¢ RESOLUTION + CTA: Cenas finais ‚Äî resolu√ß√£o e fechamento
- Cada segmento pode cobrir m√∫ltiplas cenas (a dura√ß√£o real ser√° calculada automaticamente)
- M√°ximo de 38 cenas por track (190s / 5s por cena = limite do modelo Stable Audio)
- Use varia√ß√µes sutis da mesma base musical por segmento
- Exemplos de tracks (para um v√≠deo de 60 cenas):
  ‚Ä¢ { prompt: "Cinematic, Impact Drums, Brass Stabs, Tension, Attention-Grabbing, Epic, 120 BPM", volume: -12, startScene: 0, endScene: 2 }
  ‚Ä¢ { prompt: "Cinematic, Building Strings, Crescendo, Tension Build-Up, Suspenseful, 100 BPM", volume: -12, startScene: 3, endScene: 8 }
  ‚Ä¢ { prompt: "Cinematic, Full Orchestra, Emotional Peak, Dramatic, Powerful, Climactic, 130 BPM", volume: -10, startScene: 9, endScene: null }`
    }

    return `Voc√™ √© um roteirista mestre em storytelling cinematogr√°fico e reten√ß√£o viral.

---
ESTILO NARRATIVO E PERSONA:
${styleInstructions}

---
üìê ARQUITETURA NARRATIVA PROPORCIONAL (OBRIGAT√ìRIO):
O roteiro DEVE seguir propor√ß√µes r√≠gidas entre seus atos. Isso √© CR√çTICO para manter a reten√ß√£o do in√≠cio ao fim.

| FASE | PROPOR√á√ÉO DO TOTAL | FUN√á√ÉO |
|------|-------|--------|
| üéØ HOOK (Gancho) | ‚â§5% das cenas | Captura aten√ß√£o. Mist√©rio + Promessa. In media res. |
| üìú CORPO FACTUAL (Investiga√ß√£o) | 55-65% das cenas | Fatos, cronologia, revela√ß√µes, evid√™ncias. O CORA√á√ÉO do v√≠deo. |
| üîó PONTE TEMPORAL (se aplic√°vel) | 10-15% das cenas | Conex√£o passado-presente, relev√¢ncia contempor√¢nea. |
| üí° REFLEX√ÉO/LI√á√ÉO | ‚â§15% das cenas | Significado, implica√ß√£o, questionamento. CONCISO e IMPACTANTE. |
| üì¢ CTA (Encerramento) | ‚â§5% das cenas (m√°x 2-3 cenas) | Seguir canal + assinatura "The Gap Files". |

üö® REGRA DE PROPOR√á√ÉO M√ÅXIMA: A se√ß√£o de REFLEX√ÉO/LI√á√ÉO (tudo depois do corpo factual e ponte temporal) NUNCA deve ultrapassar 20% do total de cenas. Se o v√≠deo tem 150 cenas, a reflex√£o deve ter NO M√ÅXIMO 30 cenas. Prefira 15-20%. Roteiros com reflex√£o longa demais causam QUEDA DE RETEN√á√ÉO.

üö® REGRA ANTI-REPETI√á√ÉO (CR√çTICO):
- PROIBIDO repetir a mesma ideia com varia√ß√µes. Se j√° disse "uma mentira de 500 anos", N√ÉO repita como "uma fake news medieval", "a mesma narrativa secular", etc.
- Cada cena deve avan√ßar o argumento ou adicionar informa√ß√£o NOVA. Se n√£o tem conte√∫do novo, a cena n√£o deveria existir.
- T√âCNICA: Use a regra "1 ideia = 1 cena". Se a ideia j√° foi expressa, avance para a pr√≥xima.
- A reflex√£o final deve ser CIR√öRGICA: poucos golpes precisos > muitas repeti√ß√µes dilu√≠das.
- PREFER√çVEL: Uma reflex√£o de 5 cenas devastadoras a uma reflex√£o de 25 cenas repetitivas.

---
DIRETRIZES T√âCNICAS (CR√çTICO):
- SINCRONIA: Cada cena DEVE durar EXATAMENTE 5 segundos de narra√ß√£o.
- DENSIDADE OBRIGAT√ìRIA: Com base na velocidade de fala (${targetWPM} WPM), cada cena DEVE conter entre ${wordsPerScene - 1} e ${maxWordsHard} palavras. A conta √©: ${targetWPM} WPM √∑ 60 √ó 5s = ${wordsPerScene} palavras ideais.
- üö® HARD LIMIT: NUNCA exceda ${maxWordsHard} palavras por cena. Cenas com mais de ${maxWordsHard} palavras ultrapassam 5 segundos e quebram a sincronia do v√≠deo.
- PROIBIDO FRASES CURTAS: Cenas com menos de ${wordsPerScene - 1} palavras geram "buracos" no √°udio. Expanda com adjetivos, detalhes sensoriais ou contexto.
- FLUIDEZ: O texto deve preencher exatamente 5 segundos de fala cont√≠nua. Nem mais, nem menos.
- SOUND DESIGN: Descreva a atmosfera sonora (SFX/Ambience) em ingl√™s t√©cnico para cada cena.
- M√öSICA DE FUNDO: Use "backgroundMusic" para v√≠deos curtos (TikTok/Instagram) ou "backgroundMusicTracks" para v√≠deos longos (YouTube). O campo "prompt" deve ser compat√≠vel com Stable Audio 2.5 (g√™nero, instrumentos, BPM, mood). O campo "volume" (dB) ser√° aplicado via FFmpeg na mixagem.
- CAMADA SENSORIAL: Nas descri√ß√µes visuais, inclua sentimentos, texturas e atmosfera.
- DIRETRIZ VISUAL: As descri√ß√µes visuais ('visualDescription') DEVEM SER SEMPRE EM INGL√äS e COMPLETAS: cada uma √© o prompt final que ser√° enviado diretamente ao modelo de imagem, sem etapa de merge. Incorpore estilo (base, ilumina√ß√£o, atmosfera, composi√ß√£o) em linguagem natural em cada cena. Seja VISUAL e CINEMATOGR√ÅFICO ‚Äî descreva cenas com impacto m√°ximo, texturas, atmosferas sombrias e composi√ß√µes dram√°ticas. Use linguagem art√≠stica intensa sem filtrar o tom.
- PERSONAGENS: Quando houver personagens recorrentes na narrativa, use SEMPRE os nomes (ou um descritor consistente, ex.: "the detective", "Maria") no visualDescription em todas as cenas em que aparecem. Isso reduz varia√ß√£o entre cenas e ajuda a manter coer√™ncia visual (ex.: "John standing by the window" em vez de "a man by the window").
- CONSIST√äNCIA VISUAL DE PERSONAGENS: Quando o dossi√™ fornecer visualDescription para personagens-chave, incorpore EXATAMENTE esses descritores visuais no visualDescription de cada cena onde o personagem aparece. Isso garante que o modelo de imagem mantenha a mesma apar√™ncia entre cenas.
- MULTIMODALIDADE: Se imagens forem fornecidas, analise-as para garantir consist√™ncia visual.
- CENAS DE ENCERRAMENTO (CTA ‚Äî OBRIGAT√ìRIO): As √∫ltimas cenas do v√≠deo (segmento CTA do plano narrativo) DEVEM incluir: (1) uma frase de gatilho para o espectador seguir o canal ‚Äî por exemplo convite para se inscrever, ativar o sininho ou acompanhar o canal, no tom do v√≠deo; (2) men√ß√£o ao canal "The Gap Files" como assinatura de encerramento. A hist√≥ria narrativa deve estar COMPLETAMENTE encerrada antes do CTA ‚Äî nunca corte uma frase no meio na √∫ltima cena de conte√∫do. Reserve as √∫ltimas 1-2 cenas exclusivamente para conclus√£o da frase/ideia e CTA.
${musicInstructions}

---
${visualInstructions}`
  }

  private buildUserPrompt(request: ScriptGenerationRequest): string {
    const targetWPM = request.targetWPM || 150
    const wordsPerScene = Math.round((targetWPM / 60) * 5) // 150 WPM = 12-13 palavras por 5s
    const minWords = wordsPerScene - 1
    const maxWords = wordsPerScene + 2 // Hard limit para n√£o ultrapassar 5s
    const idealSceneCount = Math.ceil(request.targetDuration / 5)
    const maxExtraScenes = 4 // margem para concluir a hist√≥ria e CTA sem cortar frase
    const maxSceneCount = idealSceneCount + maxExtraScenes

    // Determinar formato do v√≠deo
    const videoFormat = request.format || request.outputType || 'full-youtube'
    const isShortFormat = videoFormat.includes('tiktok') || videoFormat.includes('reels') || videoFormat.includes('teaser')
    const isYouTubeCinematic = videoFormat.includes('youtube') || videoFormat.includes('full')

    let formatContext = ''
    if (isShortFormat) {
      formatContext = `\n\nüì± FORMATO DO V√çDEO: TikTok/Instagram (v√≠deo curto, 30-180s)
üö® REGRA CR√çTICA DE M√öSICA DE FUNDO:
- Use o campo "backgroundMusic" com { prompt, volume } para definir UMA m√∫sica para TODO o v√≠deo
- O "prompt" deve ser compat√≠vel com Stable Audio 2.5 (g√™nero, instrumentos, BPM, mood)
- O "volume" deve ser em dB (-24 a -6) para mixagem com narra√ß√£o`
    } else if (isYouTubeCinematic) {
      formatContext = `\n\nüé¨ FORMATO DO V√çDEO: YouTube Cinematic (v√≠deo longo, 600-3600s)
- Use a lista "backgroundMusicTracks" com tracks { prompt, volume, startScene, endScene }
- Cada track referencia CENAS (n√£o timestamps). A dura√ß√£o real ser√° calculada automaticamente.
- Agrupe cenas por segmentos narrativos (HOOK, CONTEXT, RISING ACTION, CLIMAX, RESOLUTION, CTA)
- M√∫sica pode variar por segmento narrativo, mas N√ÉO fa√ßa uma track por cena`
    }

    let baseInstruction = `Crie um roteiro em ${request.language} sobre o tema: "${request.theme}"${formatContext}`

    if (request.dossierCategory) {
      baseInstruction += `\n\nüè∑Ô∏è CLASSIFICA√á√ÉO TEM√ÅTICA: ${request.dossierCategory.toUpperCase()}`
      if (request.musicGuidance) {
        baseInstruction += `\nüéµ ORIENTA√á√ÉO MUSICAL PARA ESTA CLASSIFICA√á√ÉO: O prompt de m√∫sica DEVE seguir esta dire√ß√£o: "${request.musicGuidance}"`
        baseInstruction += `\nüíì ATMOSFERA EMOCIONAL DA TRILHA: ${request.musicMood}`
        baseInstruction += `\nUse esta orienta√ß√£o como BASE para os prompts de backgroundMusic/backgroundMusicTracks. Adapte conforme o tom do roteiro, mas mantenha a ess√™ncia da classifica√ß√£o.`
      }
      if (request.visualGuidance) {
        baseInstruction += `\n\nüñºÔ∏è ORIENTA√á√ÉO VISUAL (visualDescription): As descri√ß√µes visuais de cada cena DEVEM seguir este tom e regras: ${request.visualGuidance}`
        baseInstruction += `\nAplique esta orienta√ß√£o em TODAS as cenas. O visualDescription deve ser pronto para gera√ß√£o de imagem e alinhado ao tema do v√≠deo.`
      }
    }

    // Fontes do dossi√™ (arquitetura flat/democratizada)
    const allSources = request.sources || request.additionalSources || []
    if (allSources.length > 0) {
      baseInstruction += `\n\nüìö FONTES DO DOSSI√ä (BASE NEURAL):`
      allSources.forEach((source, index) => {
        baseInstruction += `\n[üìÑ FONTE ${index + 1}] (${source.type}): ${source.title}\n${source.content}\n---`
      })
    }

    if (request.userNotes && request.userNotes.length > 0) {
      baseInstruction += `\n\nüß† INSIGHTS E NOTAS DO AGENTE:\n${request.userNotes.join('\n- ')}`
    }

    // Persons & Neural Insights (Intelligence Center)
    const personsBlock = formatPersonsForPrompt(request.persons || [])
    if (personsBlock) {
      baseInstruction += `\n\n${personsBlock}`
    }
    const insightsBlock = formatNeuralInsightsForPrompt(request.neuralInsights || [])
    if (insightsBlock) {
      baseInstruction += `\n\n${insightsBlock}`
    }

    if (request.visualReferences && request.visualReferences.length > 0) {
      baseInstruction += `\n\nüñºÔ∏è REFER√äNCIAS VISUAIS EXISTENTES (DESCRITORES):\n${request.visualReferences.join('\n- ')}`
    }

    if (request.researchData) {
      baseInstruction += `\n\nüìä DADOS ESTAT√çSTICOS/ESTRUTURADOS:\n${JSON.stringify(request.researchData, null, 2)}`
    }

    if (request.storyOutline) {
      baseInstruction += `\n\n${request.storyOutline}`
    }

    if (request.additionalContext) {
      baseInstruction += `\n\n‚ûï CONTEXTO ADICIONAL:\n${request.additionalContext}`
    }

    let guidelines = ''
    if (request.mustInclude) guidelines += `\n- DEVE INCLUIR: ${request.mustInclude}`
    if (request.mustExclude) guidelines += `\n- N√ÉO PODE CONTER: ${request.mustExclude}`

    let musicWarning = ''
    if (isShortFormat) {
      musicWarning = `\n\nüö® REGRA CR√çTICA DE M√öSICA DE FUNDO (TikTok/Instagram):
Use "backgroundMusic": { "prompt": "...", "volume": -12 } para definir UMA m√∫sica para TODO o v√≠deo (prefira volume entre -12 e -10 para ficar aud√≠vel).
O prompt deve seguir o formato Stable Audio 2.5: g√™nero, instrumentos, BPM, mood.
Defina "backgroundMusicTracks" como null.`
    } else if (isYouTubeCinematic) {
      musicWarning = `\n\nüö® REGRA CR√çTICA DE M√öSICA DE FUNDO (YouTube Cinematic):
Use "backgroundMusicTracks" com lista de tracks { prompt, volume, startScene, endScene }.
"startScene" e "endScene" s√£o N√öMEROS DE CENA (0-indexed), N√ÉO timestamps em segundos.
O prompt de cada track deve seguir o formato Stable Audio 2.5: g√™nero, instrumentos, BPM, mood.
M√°ximo de 38 cenas por track (limite do modelo). Defina "backgroundMusic" como null.`
    }

    const maxReflectionScenes = Math.max(3, Math.round(idealSceneCount * 0.15))
    const maxReflectionCeiling = Math.round(idealSceneCount * 0.20)

    return `${baseInstruction}

---
‚ö†Ô∏è REQUISITOS OBRIGAT√ìRIOS PARA APROVA√á√ÉO:
1. DURA√á√ÉO M√çNIMA: O v√≠deo deve ter pelo menos ${request.targetDuration} segundos (${idealSceneCount} cenas). Voc√™ PODE gerar at√© ${maxSceneCount} cenas (no m√°ximo ${maxExtraScenes} cenas extras) para concluir a hist√≥ria e o CTA sem cortar frases.
2. QUANTIDADE DE CENAS: Gere entre ${idealSceneCount} e ${maxSceneCount} cenas. Use as cenas extras APENAS para: (a) terminar a √∫ltima ideia/frase da hist√≥ria sem cortar no meio; (b) incluir o CTA completo (convite para seguir o canal + men√ß√£o The Gap Files). N√£o extrapole al√©m de ${maxSceneCount} cenas.
3. DURA√á√ÉO DA CENA: Cada cena tem slots fixos de 5 segundos.
4. CONTAGEM DE PALAVRAS: Cada narra√ß√£o DEVE ter entre ${minWords} e ${maxWords} palavras (${targetWPM} WPM √∑ 60 √ó 5s = ${wordsPerScene} palavras ideais). üö® NUNCA exceda ${maxWords} palavras - isso faz o √°udio ultrapassar 5 segundos e quebra a sincronia. NUNCA fa√ßa cenas com menos de ${minWords} palavras - isso gera sil√™ncio.
5. M√öSICA DE FUNDO: ${isShortFormat ? 'Use "backgroundMusic" { prompt, volume } para UMA m√∫sica para TODO o v√≠deo. O prompt deve ser compat√≠vel com Stable Audio 2.5.' : 'Use "backgroundMusicTracks" com tracks { prompt, volume, startTime, endTime }. O prompt de cada track deve ser compat√≠vel com Stable Audio 2.5.'}
6. Se houver imagens anexas, use-as como refer√™ncia visual prim√°ria.
7. üìê PROPOR√á√ÉO NARRATIVA: A se√ß√£o de REFLEX√ÉO/LI√á√ÉO (ap√≥s o corpo factual + ponte temporal) deve ter no M√ÅXIMO ${maxReflectionScenes} cenas (15% ideal, ${maxReflectionCeiling} cenas = teto absoluto de 20%). Invista as cenas no CORPO FACTUAL, n√£o na reflex√£o.
8. üö´ ANTI-REPETI√á√ÉO: Antes de finalizar, releia TODAS as cenas de reflex√£o. Se duas cenas expressam a mesma ideia com palavras diferentes, ELIMINE uma e redistribua o conte√∫do para o corpo factual. Cada cena de reflex√£o deve trazer um ARGUMENTO √öNICO e IN√âDITO.
${guidelines}${musicWarning}

ÔøΩ VALIDA√á√ÉO FINAL OBRIGAT√ìRIA:
Antes de retornar o JSON, fa√ßa esta auditoria interna:
1. CONTE as cenas totais ‚Äî deve estar entre ${idealSceneCount} e ${maxSceneCount}.
2. CONTE as cenas de reflex√£o/li√ß√£o (ap√≥s o corpo factual) ‚Äî deve ser ‚â§${maxReflectionCeiling} cenas.
3. PROCURE repeti√ß√µes tem√°ticas ‚Äî se encontrar, ELIMINE e COMPACTE.
4. A √∫ltima cena de conte√∫do deve terminar com frase completa.
5. As √∫ltimas 1-2 cenas devem ser conclus√£o + CTA (seguir canal + The Gap Files).`
  }

  private parseResponse(
    content: ScriptResponse,
    request: ScriptGenerationRequest,
    tokenUsage?: { inputTokens: number; outputTokens: number; totalTokens: number }
  ): ScriptGenerationResponse {
    const scenes: ScriptScene[] = content.scenes.map((scene, index) => ({
      order: scene.order ?? index + 1,
      narration: scene.narration,
      visualDescription: scene.visualDescription,
      audioDescription: scene.audioDescription ?? undefined,
      estimatedDuration: scene.estimatedDuration ?? 5
    }))

    const fullText = scenes.map(s => s.narration).join('\n\n')
    const wordCount = fullText.split(/\s+/).length
    const estimatedDuration = scenes.reduce((acc, s) => acc + s.estimatedDuration, 0)

    const usage = tokenUsage ?? { inputTokens: 0, outputTokens: 0, totalTokens: 0 }
    return {
      title: content.title,
      summary: content.summary,
      fullText,
      scenes,
      backgroundMusic: content.backgroundMusic ?? undefined,
      backgroundMusicTracks: content.backgroundMusicTracks ?? undefined,
      wordCount,
      estimatedDuration,
      provider: this.getName(),
      model: this.modelName,
      usage: tokenUsage,
      costInfo: {
        cost: calculateLLMCost(this.modelName, usage.inputTokens, usage.outputTokens),
        provider: this.getName(),
        model: this.modelName,
        metadata: {
          input_tokens: usage.inputTokens,
          output_tokens: usage.outputTokens,
          total_tokens: usage.totalTokens
        }
      }
    }
  }
}
