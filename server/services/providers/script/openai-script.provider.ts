import { ChatOpenAI } from '@langchain/openai'
import {
  SystemMessage,
  HumanMessage,
  BaseMessage
} from '@langchain/core/messages'
import type {
  IScriptGenerator,
  ScriptGenerationRequest,
  ScriptGenerationResponse
} from '../../../types/ai-providers'
import {
  ScriptResponseSchema,
  type ScriptResponse,
  buildSystemPrompt,
  buildUserPrompt,
  parseScriptResponse,
  processImagesForLangChain,
  fallbackParseRawResponse,
  extractTokenUsage
} from './shared-script-prompts'

export class OpenAIScriptProvider implements IScriptGenerator {
  private model: ChatOpenAI
  private modelName: string

  constructor(config: { apiKey: string; model?: string; baseUrl?: string; temperature?: number }) {
    this.modelName = config.model ?? 'gpt-4o'
    this.model = new ChatOpenAI({
      openAIApiKey: config.apiKey,
      modelName: this.modelName,
      configuration: {
        baseURL: config.baseUrl ?? 'https://api.openai.com/v1'
      },
      temperature: config.temperature ?? 0.5, // Do LlmAssignment (script task)
      timeout: 120000, // 2 minutos de timeout para chamadas multimodais complexas
      maxRetries: 2
    })
  }

  getName(): string {
    return 'OPENAI'
  }

  async generate(request: ScriptGenerationRequest): Promise<ScriptGenerationResponse> {
    const LOG = '[OpenAI Script]'
    console.log(`${LOG} üé¨ Iniciando gera√ß√£o de roteiro via LangChain (${this.modelName})...`)

    const m = this.model as any
    // Groq Llama 4: for√ßar jsonMode (SDK n√£o suporta jsonSchema para Llama).
    // Groq GPT-OSS: SDK autodetecta jsonSchema pelo prefixo 'openai/gpt-oss' ‚Üí sem override.
    const isGroqLlama4 = (m.constructor?.name === 'ChatGroq' || this.modelName.includes('llama-4')) && !this.modelName.startsWith('openai/')
    const structuredLlm = typeof m.withStructuredOutputReplicate === 'function'
      ? m.withStructuredOutputReplicate(ScriptResponseSchema, { includeRaw: true })
      : m.withStructuredOutput(ScriptResponseSchema, { includeRaw: true, ...(isGroqLlama4 ? { method: 'jsonMode' } : {}) })

    const systemPrompt = buildSystemPrompt(request)
    const userPrompt = buildUserPrompt(request, 'openai')

    // Log para depura√ß√£o
    console.log('--- [DEBUG] LANGCHAIN CONFIGURATION ---')
    console.log('Target Duration:', request.targetDuration, 'seconds')
    console.log('Target WPM:', request.targetWPM)
    console.log('Ideal Scene Count:', Math.ceil(request.targetDuration / 5))
    console.log('--- [DEBUG] LANGCHAIN SYSTEM PROMPT ---')
    console.log(systemPrompt)

    // Preparar mensagens (Suporte Multimodal)
    const messages: BaseMessage[] = [new SystemMessage(systemPrompt)]

    const humanContent: any[] = [{ type: 'text', text: userPrompt }]

    // Injetar imagens se dispon√≠veis (Vision Capability)
    const imageParts = processImagesForLangChain(request.images, LOG)
    // OpenAI suporta detail: 'high'
    imageParts.forEach(p => { p.image_url.detail = 'high' })
    humanContent.push(...imageParts)

    messages.push(new HumanMessage({ content: humanContent }))

    try {
      const startTime = Date.now()
      console.log(`${LOG} üì§ Enviando request multimodal para LangChain...`)
      console.log(`${LOG} üîç Schema esperado: title, summary, scenes, backgroundMusic, backgroundMusicTracks`)

      const { invokeWithLogging } = await import('../../../utils/llm-invoke-wrapper')
      const result = await invokeWithLogging(structuredLlm, messages, { taskId: 'script-openai', provider: 'openai', model: 'unknown' })
      let content = result.parsed as ScriptResponse | null
      const rawMessage = result.raw as any

      const elapsed = ((Date.now() - startTime) / 1000).toFixed(2)
      console.log(`${LOG} üì• Resposta recebida em ${elapsed}s`)

      const tokenUsage = extractTokenUsage(rawMessage)
      console.log(`${LOG} üìä Token Usage REAL: ${tokenUsage.inputTokens} input + ${tokenUsage.outputTokens} output = ${tokenUsage.totalTokens} total`)

      // FALLBACK: Se withStructuredOutput n√£o conseguiu parsear (Zod v4 compat)
      if (!content) {
        console.warn(`${LOG} ‚ö†Ô∏è result.parsed √© null ‚Äî tentando fallback manual de parsing (Zod v4 compat)...`)
        content = fallbackParseRawResponse(rawMessage, LOG)
        if (!content) {
          throw new Error('Roteiro n√£o p√¥de ser parseado. O modelo retornou dados que n√£o correspondem ao schema esperado.')
        }
      }

      // Valida√ß√£o r√°pida de integridade
      console.log(`${LOG} ‚úÖ Roteiro gerado com sucesso!`)
      console.log(`${LOG} T√≠tulo:`, content.title)
      console.log(`${LOG} N√∫mero de cenas:`, content.scenes.length)
      console.log(`${LOG} Background Music:`, content.backgroundMusic ? 'Sim (video todo)' : 'N√£o')
      console.log(`${LOG} Background Music Tracks:`, content.backgroundMusicTracks?.length || 0, 'tracks')

      return parseScriptResponse(content, request, this.getName(), this.modelName, tokenUsage)
    } catch (error) {
      console.error(`${LOG} ‚ùå Erro na gera√ß√£o estruturada:`, error)
      console.error(`${LOG} üîç Error details:`, JSON.stringify(error, null, 2))
      throw error
    }
  }
}
