import { z } from 'zod'
import { ChatAnthropic } from '@langchain/anthropic'
import {
  SystemMessage,
  HumanMessage,
  BaseMessage
} from '@langchain/core/messages'
import type {
  IScriptGenerator,
  ScriptGenerationRequest,
  ScriptGenerationResponse,
  ScriptScene
} from '../../../types/ai-providers'
import { buildVisualInstructionsForScript } from '../../../utils/wan-prompt-builder'

// Schema para valida√ß√£o estruturada do output (Garante JSON v√°lido e tipos corretos)
const ScriptSceneSchema = z.object({
  order: z.number().describe('A ordem sequencial da cena'),
  narration: z.string().describe('O texto que ser√° narrado pelo locutor'),
  visualDescription: z.string().describe('Descri√ß√£o t√©cnica e sensorial para o modelo de gera√ß√£o de v√≠deo (SEMPRE EM INGL√äS)'),
  audioDescription: z.string().nullable().describe('Atmosfera sonora e SFX em ingl√™s t√©cnico'),
  estimatedDuration: z.number().describe('Dura√ß√£o estimada em segundos (entre 5 e 6 segundos)')
})

const BackgroundMusicTrackSchema = z.object({
  prompt: z.string().describe('Prompt para gera√ß√£o de m√∫sica no formato Stable Audio. Inclua g√™nero, instrumentos, BPM, mood e estilo. Exemplo: "Ambient, Drone, Dark Strings, Pulsing Heartbeat Rhythm, Tension Build-Up, Mysterious, Cinematic, Atmospheric, 80 BPM"'),
  volume: z.number().describe('Volume em dB para mixagem com narra√ß√£o. Use valores entre -24 e -6. Exemplo: -18 para volume baixo, -12 para m√©dio, -6 para alto'),
  startTime: z.number().describe('Tempo de in√≠cio em segundos (0 = in√≠cio do v√≠deo)'),
  endTime: z.number().nullable().describe('Tempo de fim em segundos (null = at√© o final do v√≠deo)')
})

const BackgroundMusicSchema = z.object({
  prompt: z.string().describe('Prompt para gera√ß√£o de m√∫sica no formato Stable Audio. Inclua g√™nero, instrumentos, BPM, mood e estilo. Exemplo: "Ambient, Drone, Dark Strings, Subtle Pads, Mysterious, Cinematic, Atmospheric, well-arranged composition, 80 BPM"'),
  volume: z.number().describe('Volume em dB para mixagem com narra√ß√£o. Use valores entre -24 e -6. Exemplo: -18 para volume baixo, -12 para m√©dio, -6 para alto')
})

const ScriptResponseSchema = z.object({
  title: z.string().describe('T√≠tulo impactante para o v√≠deo'),
  summary: z.string().describe('Sinopse intrigante de 2-3 par√°grafos'),
  scenes: z.array(ScriptSceneSchema).describe('Lista de cenas que comp√µem o v√≠deo'),
  backgroundMusic: BackgroundMusicSchema.nullable().describe('M√∫sica de fundo √∫nica para TODO o v√≠deo (use apenas para v√≠deos curtos TikTok/Instagram). Use null para v√≠deos longos. Regra: "video todo"'),
  backgroundMusicTracks: z.array(BackgroundMusicTrackSchema).nullable().describe('Lista de tracks de m√∫sica de fundo com timestamps (use apenas para v√≠deos longos YouTube Cinematic). Use null para v√≠deos curtos. Cada track define uma m√∫sica com prompt, volume e timestamps.')
})

type ScriptResponse = z.infer<typeof ScriptResponseSchema>

export class AnthropicScriptProvider implements IScriptGenerator {
  private model: ChatAnthropic
  private modelName: string

  constructor(config: { apiKey: string; model?: string }) {
    this.modelName = config.model ?? 'claude-opus-4-6'
    this.model = new ChatAnthropic({
      anthropicApiKey: config.apiKey,
      modelName: this.modelName,
      temperature: 0.7,
      maxTokens: 16384, // Anthropic exige maxTokens expl√≠cito
      clientOptions: {
        timeout: 180000, // 3 minutos -- Opus √© mais lento que GPT-4o
        maxRetries: 2
      }
    })
  }

  getName(): string {
    return 'ANTHROPIC'
  }

  async generate(request: ScriptGenerationRequest): Promise<ScriptGenerationResponse> {
    console.log('[Anthropic Script] üé¨ Iniciando gera√ß√£o de roteiro via LangChain + Claude...')

    // Configurar o modelo para output estruturado (Zod) com includeRaw para capturar token usage
    const structuredLlm = this.model.withStructuredOutput(ScriptResponseSchema, { includeRaw: true })

    const systemPrompt = this.buildSystemPrompt(request)
    const userPrompt = this.buildUserPrompt(request)

    // Log para depura√ß√£o
    console.log('--- [DEBUG] LANGCHAIN ANTHROPIC CONFIGURATION ---')
    console.log('Model:', this.modelName)
    console.log('Target Duration:', request.targetDuration, 'seconds')
    console.log('Target WPM:', request.targetWPM)
    console.log('Ideal Scene Count:', Math.ceil(request.targetDuration / 5))
    console.log('--- [DEBUG] LANGCHAIN SYSTEM PROMPT ---')
    console.log(systemPrompt)

    // Preparar mensagens (Suporte Multimodal)
    const messages: BaseMessage[] = [
      new SystemMessage(systemPrompt)
    ]

    // Construir conte√∫do da mensagem do usu√°rio (Texto + Imagens)
    const humanContent: any[] = [
      { type: 'text', text: userPrompt }
    ]

    // Injetar imagens se dispon√≠veis (Claude Vision)
    if (request.images && request.images.length > 0) {
      console.log(`[Anthropic Script] üëÅÔ∏è Injetando ${request.images.length} imagens no contexto multimodal...`)

      request.images.forEach((img, idx) => {
        let base64Data = ''

        try {
          if (Buffer.isBuffer(img.data)) {
            base64Data = img.data.toString('base64')
          } else if (typeof img.data === 'string') {
            base64Data = img.data
          } else if (typeof img.data === 'object') {
            if ((img.data as any).type === 'Buffer' && Array.isArray((img.data as any).data)) {
              base64Data = Buffer.from((img.data as any).data).toString('base64')
            } else {
              base64Data = Buffer.from(img.data as any).toString('base64')
            }
          }
        } catch (e) {
          console.warn(`[Anthropic Script] ‚ö†Ô∏è Falha ao converter imagem ${idx}. Erro: ${e}`)
        }

        if (!base64Data) {
          console.warn(`[Anthropic Script] ‚ö†Ô∏è Imagem ${idx} ignorada: falha na extra√ß√£o de dados. Tipo: ${typeof img.data}`)
          return
        }

        // Remover prefixo data:image/...;base64, se j√° existir
        if (base64Data.includes('base64,')) {
          base64Data = base64Data.split('base64,')[1] || ''
        }

        const validMimeTypes = ['image/jpeg', 'image/png', 'image/gif', 'image/webp']
        let mimeType = (img.mimeType || 'image/jpeg').toLowerCase()
        if (mimeType === 'image/jpg') mimeType = 'image/jpeg'

        if (!validMimeTypes.includes(mimeType)) {
          console.warn(`[Anthropic Script] ‚ö†Ô∏è Imagem ${idx} ignorada: formato n√£o suportado (${mimeType}).`)
          return
        }

        // Formato de imagem para Claude via LangChain (image_url com data URI)
        humanContent.push({
          type: 'image_url',
          image_url: {
            url: `data:${mimeType};base64,${base64Data}`
          }
        })
      })
    }

    messages.push(new HumanMessage({ content: humanContent }))

    try {
      const startTime = Date.now()
      console.log('[Anthropic Script] üì§ Enviando request multimodal para LangChain + Claude...')
      console.log('[Anthropic Script] üîç Schema esperado: title, summary, scenes, backgroundMusic, backgroundMusicTracks')

      const result = await structuredLlm.invoke(messages)
      const content = result.parsed as ScriptResponse
      const rawMessage = result.raw as any

      const elapsed = ((Date.now() - startTime) / 1000).toFixed(2)
      console.log(`[Anthropic Script] üì• Resposta recebida e validada em ${elapsed}s`)

      // Extrair token usage real da resposta
      const usage = rawMessage?.usage_metadata || rawMessage?.response_metadata?.usage
      const inputTokens = usage?.input_tokens ?? 0
      const outputTokens = usage?.output_tokens ?? 0
      const totalTokens = usage?.total_tokens ?? (inputTokens + outputTokens)

      console.log(`[Anthropic Script] üìä Token Usage REAL: ${inputTokens} input + ${outputTokens} output = ${totalTokens} total`)

      // Valida√ß√£o r√°pida de integridade
      console.log('[Anthropic Script] ‚úÖ Roteiro gerado com sucesso!')
      console.log('[Anthropic Script] T√≠tulo:', content.title)
      console.log('[Anthropic Script] N√∫mero de cenas:', content.scenes.length)
      console.log('[Anthropic Script] Background Music:', content.backgroundMusic ? 'Sim (video todo)' : 'N√£o')
      console.log('[Anthropic Script] Background Music Tracks:', content.backgroundMusicTracks?.length || 0, 'tracks')

      return this.parseResponse(content, request, { inputTokens, outputTokens, totalTokens })
    } catch (error) {
      console.error('[Anthropic Script] ‚ùå Erro na gera√ß√£o estruturada:', error)
      console.error('[Anthropic Script] üîç Error details:', JSON.stringify(error, null, 2))
      throw error
    }
  }

  private buildSystemPrompt(request: ScriptGenerationRequest): string {
    let styleInstructions = request.scriptStyleInstructions || 'Adote um tom documental s√©rio e investigativo.'

    let visualInstructions = ''
    if (request.visualBaseStyle) {
      visualInstructions = buildVisualInstructionsForScript({
        baseStyle: request.visualBaseStyle,
        lightingTags: request.visualLightingTags || '',
        atmosphereTags: request.visualAtmosphereTags || '',
        compositionTags: request.visualCompositionTags || '',
        generalTags: request.visualGeneralTags
      })
    } else if (request.visualStyleDescription) {
      visualInstructions = `DIRETRIZ VISUAL OBRIGAT√ìRIA: ${request.visualStyleDescription}`
    } else {
      visualInstructions = buildVisualInstructionsForScript({
        baseStyle: 'Cinematic Mystery Documentary',
        lightingTags: 'Chiaroscuro, dramatic volumetric lighting, shadows dancing',
        atmosphereTags: 'Mysterious, moody, foggy, dense atmosphere',
        compositionTags: 'Cinematic wide shots, extreme close-ups on textures',
        generalTags: '4k, highly detailed, realistic textures, grainy film look'
      })
    }

    const targetWPM = request.targetWPM || 150
    const wordsPerScene = Math.round((targetWPM / 60) * 5)
    const maxWordsHard = wordsPerScene + 2
    const wordRange = `${wordsPerScene - 1}-${wordsPerScene + 1}`

    // Determinar formato do v√≠deo para instru√ß√µes de m√∫sica
    const videoFormat = request.format || request.outputType || 'full-youtube'
    const isShortFormat = videoFormat.includes('tiktok') || videoFormat.includes('reels') || videoFormat.includes('teaser')
    const isYouTubeCinematic = videoFormat.includes('youtube') || videoFormat.includes('full')

    let musicInstructions = ''
    if (isShortFormat) {
      musicInstructions = `
---
üéµ ESTRAT√âGIA DE M√öSICA DE FUNDO (TikTok/Instagram):
- üö® REGRA: "video todo" - Use UMA m√∫sica de fundo para TODO o v√≠deo do in√≠cio ao fim
- Use o campo "backgroundMusic" com "prompt" e "volume"
- O "prompt" ser√° usado diretamente no modelo Stable Audio 2.5 para gerar a m√∫sica
- FORMATO DO PROMPT: Inclua g√™nero, sub-g√™nero, instrumentos espec√≠ficos, BPM, mood e estilo
- O "volume" √© em dB para mixagem com narra√ß√£o (-24 a -6). Use -18 para baixo, -12 para m√©dio
- Exemplo de prompt: "Ambient, Dark Drone, Subtle Synthesizer Pads, Low Strings, Mysterious, Cinematic, Atmospheric, well-arranged composition, 80 BPM"
- N√ÉO inclua volume no prompt - o volume √© um campo separado
- Exemplo completo: { prompt: "Ambient, Dark Drone, Subtle Pads, Mysterious, Cinematic, 80 BPM", volume: -18 }`
    } else if (isYouTubeCinematic) {
      musicInstructions = `
---
üéµ ESTRAT√âGIA DE M√öSICA DE FUNDO (YouTube Cinematic):
- Use a lista "backgroundMusicTracks" para definir tracks com timestamps
- Cada track tem: "prompt" (para Stable Audio 2.5), "volume" (dB), "startTime" e "endTime"
- O "prompt" ser√° usado diretamente no modelo Stable Audio 2.5 para gerar cada track
- FORMATO DO PROMPT: Inclua g√™nero, sub-g√™nero, instrumentos espec√≠ficos, BPM, mood e estilo
- O "volume" √© em dB para mixagem com narra√ß√£o (-24 a -6). Use -18 para baixo, -12 para m√©dio
- N√ÉO mude m√∫sica a cada 5 segundos (cada cena)
- Agrupe cenas por SEGMENTOS NARRATIVOS maiores (15-60s):
  ‚Ä¢ HOOK (0-15s): M√∫sica de abertura impactante
  ‚Ä¢ CONTEXT (15-45s): Transi√ß√£o suave, estabelecimento
  ‚Ä¢ RISING ACTION: Intensidade crescente progressiva
  ‚Ä¢ CLIMAX: Pico emocional m√°ximo
  ‚Ä¢ RESOLUTION: Resolu√ß√£o e s√≠ntese
  ‚Ä¢ CTA: Fechamento apropriado
- Cada track deve ter dura√ß√£o m√°xima de 190 segundos (limite do modelo)
- Use varia√ß√µes sutis da mesma m√∫sica base por segmento
- Exemplos de tracks:
  ‚Ä¢ { prompt: "Cinematic, Impact Drums, Brass Stabs, Tension, Attention-Grabbing, Epic, 120 BPM", volume: -14, startTime: 0, endTime: 15 }
  ‚Ä¢ { prompt: "Cinematic, Building Strings, Crescendo, Tension Build-Up, Suspenseful, 100 BPM", volume: -16, startTime: 15, endTime: 45 }
  ‚Ä¢ { prompt: "Cinematic, Full Orchestra, Emotional Peak, Dramatic, Powerful, Climactic, 130 BPM", volume: -12, startTime: 45, endTime: null }`
    }

    return `Voc√™ √© um roteirista mestre em storytelling cinematogr√°fico e reten√ß√£o viral.

---
ESTILO NARRATIVO E PERSONA:
${styleInstructions}

---
DIRETRIZES T√âCNICAS (CR√çTICO):
- SINCRONIA: Cada cena DEVE durar EXATAMENTE 5 segundos de narra√ß√£o.
- DENSIDADE OBRIGAT√ìRIA: Com base na velocidade de fala (${targetWPM} WPM), cada cena DEVE conter entre ${wordsPerScene - 1} e ${maxWordsHard} palavras. A conta √©: ${targetWPM} WPM √∑ 60 √ó 5s = ${wordsPerScene} palavras ideais.
- üö® HARD LIMIT: NUNCA exceda ${maxWordsHard} palavras por cena. Cenas com mais de ${maxWordsHard} palavras ultrapassam 5 segundos e quebram a sincronia do v√≠deo.
- PROIBIDO FRASES CURTAS: Cenas com menos de ${wordsPerScene - 1} palavras geram "buracos" no √°udio. Expanda com adjetivos, detalhes sensoriais ou contexto.
- FLUIDEZ: O texto deve preencher exatamente 5 segundos de fala cont√≠nua. Nem mais, nem menos.
- SOUND DESIGN: Descreva a atmosfera sonora (SFX/Ambience) em ingl√™s t√©cnico para cada cena.
- M√öSICA DE FUNDO: Use "backgroundMusic" para v√≠deos curtos (TikTok/Instagram) ou "backgroundMusicTracks" para v√≠deos longos (YouTube). O campo "prompt" deve ser compat√≠vel com Stable Audio 2.5 (g√™nero, instrumentos, BPM, mood). O campo "volume" (dB) ser√° aplicado via FFmpeg na mixagem.
- CAMADA SENSORIAL: Nas descri√ß√µes visuais, inclua sentimentos, texturas e atmosfera.
- DIRETRIZ VISUAL: As descri√ß√µes visuais ('visualDescription') DEVEM SER SEMPRE EM INGL√äS, independentemente do idioma da narra√ß√£o.
- MULTIMODALIDADE: Se imagens forem fornecidas, analise-as para garantir consist√™ncia visual.
${musicInstructions}

---
${visualInstructions}`
  }

  private buildUserPrompt(request: ScriptGenerationRequest): string {
    const targetWPM = request.targetWPM || 150
    const wordsPerScene = Math.round((targetWPM / 60) * 5)
    const minWords = wordsPerScene - 1
    const maxWords = wordsPerScene + 2
    const idealSceneCount = Math.ceil(request.targetDuration / 5)

    // Determinar formato do v√≠deo
    const videoFormat = request.format || request.outputType || 'full-youtube'
    const isShortFormat = videoFormat.includes('tiktok') || videoFormat.includes('reels') || videoFormat.includes('teaser')
    const isYouTubeCinematic = videoFormat.includes('youtube') || videoFormat.includes('full')

    let formatContext = ''
    if (isShortFormat) {
      formatContext = `\n\nüì± FORMATO DO V√çDEO: TikTok/Instagram (v√≠deo curto, 30-180s)
üö® REGRA CR√çTICA DE M√öSICA DE FUNDO:
- Use o campo "backgroundMusic" com { prompt, volume } para definir UMA m√∫sica para TODO o v√≠deo
- O "prompt" deve ser compat√≠vel com Stable Audio 2.5 (g√™nero, instrumentos, BPM, mood)
- O "volume" deve ser em dB (-24 a -6) para mixagem com narra√ß√£o`
    } else if (isYouTubeCinematic) {
      formatContext = `\n\nüé¨ FORMATO DO V√çDEO: YouTube Cinematic (v√≠deo longo, 600-3600s)
- Use a lista "backgroundMusicTracks" com tracks { prompt, volume, startTime, endTime }
- Cada track tem dura√ß√£o m√°xima de 190 segundos (limite do modelo Stable Audio)
- Identifique segmentos narrativos (HOOK, CONTEXT, RISING ACTION, CLIMAX, RESOLUTION, CTA)
- M√∫sica pode variar por segmento narrativo, mas N√ÉO a cada 5 segundos`
    }

    let baseInstruction = `Crie um roteiro em ${request.language} sobre o tema: "${request.theme}"${formatContext}`

    if (request.sourceDocument) {
      baseInstruction += `\n\nüìÑ DOCUMENTO PRINCIPAL (BASE NEURAL):\n${request.sourceDocument}`
    }

    if (request.additionalSources && request.additionalSources.length > 0) {
      baseInstruction += `\n\nüìö FONTES SECUND√ÅRIAS (VETORES DE INTELIG√äNCIA):`
      request.additionalSources.forEach((source, index) => {
        baseInstruction += `\n[FONTE ${index + 1}] (${source.type}): ${source.title}\n${source.content}\n---`
      })
    }

    if (request.userNotes && request.userNotes.length > 0) {
      baseInstruction += `\n\nüß† INSIGHTS E NOTAS DO AGENTE:\n${request.userNotes.join('\n- ')}`
    }

    if (request.visualReferences && request.visualReferences.length > 0) {
      baseInstruction += `\n\nüñºÔ∏è REFER√äNCIAS VISUAIS EXISTENTES (DESCRITORES):\n${request.visualReferences.join('\n- ')}`
    }

    if (request.researchData) {
      baseInstruction += `\n\nüìä DADOS ESTAT√çSTICOS/ESTRUTURADOS:\n${JSON.stringify(request.researchData, null, 2)}`
    }

    if (request.additionalContext) {
      baseInstruction += `\n\n‚ûï CONTEXTO ADICIONAL:\n${request.additionalContext}`
    }

    let guidelines = ''
    if (request.mustInclude) guidelines += `\n- DEVE INCLUIR: ${request.mustInclude}`
    if (request.mustExclude) guidelines += `\n- N√ÉO PODE CONTER: ${request.mustExclude}`

    let musicWarning = ''
    if (isShortFormat) {
      musicWarning = `\n\nüö® REGRA CR√çTICA DE M√öSICA DE FUNDO (TikTok/Instagram):
Use "backgroundMusic": { "prompt": "...", "volume": -18 } para definir UMA m√∫sica para TODO o v√≠deo.
O prompt deve seguir o formato Stable Audio 2.5: g√™nero, instrumentos, BPM, mood.
Defina "backgroundMusicTracks" como null.`
    } else if (isYouTubeCinematic) {
      musicWarning = `\n\nüö® REGRA CR√çTICA DE M√öSICA DE FUNDO (YouTube Cinematic):
Use "backgroundMusicTracks" com lista de tracks { prompt, volume, startTime, endTime }.
O prompt de cada track deve seguir o formato Stable Audio 2.5: g√™nero, instrumentos, BPM, mood.
Cada track tem dura√ß√£o m√°xima de 190s. Defina "backgroundMusic" como null.`
    }

    return `${baseInstruction}

---
‚ö†Ô∏è REQUISITOS OBRIGAT√ìRIOS PARA APROVA√á√ÉO:
1. DURA√á√ÉO TOTAL DO V√çDEO: O v√≠deo DEVE ter EXATAMENTE ${request.targetDuration} segundos de dura√ß√£o total.
2. QUANTIDADE DE CENAS: Gere EXATAMENTE ${idealSceneCount} cenas (${request.targetDuration}s √∑ 5s por cena = ${idealSceneCount} cenas).
3. DURA√á√ÉO DA CENA: Cada cena tem slots fixos de 5 segundos.
4. CONTAGEM DE PALAVRAS: Cada narra√ß√£o DEVE ter entre ${minWords} e ${maxWords} palavras (${targetWPM} WPM √∑ 60 √ó 5s = ${wordsPerScene} palavras ideais). üö® NUNCA exceda ${maxWords} palavras - isso faz o √°udio ultrapassar 5 segundos e quebra a sincronia. NUNCA fa√ßa cenas com menos de ${minWords} palavras - isso gera sil√™ncio.
5. M√öSICA DE FUNDO: ${isShortFormat ? 'Use "backgroundMusic" { prompt, volume } para UMA m√∫sica para TODO o v√≠deo. O prompt deve ser compat√≠vel com Stable Audio 2.5.' : 'Use "backgroundMusicTracks" com tracks { prompt, volume, startTime, endTime }. O prompt de cada track deve ser compat√≠vel com Stable Audio 2.5.'}
6. Se houver imagens anexas, use-as como refer√™ncia visual prim√°ria.
${guidelines}${musicWarning}

üö® CR√çTICO: O v√≠deo final PRECISA ter ${request.targetDuration} segundos. N√£o gere menos cenas do que ${idealSceneCount}. Se necess√°rio, divida o conte√∫do em mais cenas para atingir a dura√ß√£o exata.`
  }

  private parseResponse(
    content: ScriptResponse,
    request: ScriptGenerationRequest,
    tokenUsage?: { inputTokens: number; outputTokens: number; totalTokens: number }
  ): ScriptGenerationResponse {
    const scenes: ScriptScene[] = content.scenes.map((scene, index) => ({
      order: scene.order ?? index + 1,
      narration: scene.narration,
      visualDescription: scene.visualDescription,
      audioDescription: scene.audioDescription ?? undefined,
      estimatedDuration: scene.estimatedDuration ?? 5
    }))

    const fullText = scenes.map(s => s.narration).join('\n\n')
    const wordCount = fullText.split(/\s+/).length
    const estimatedDuration = scenes.reduce((acc, s) => acc + s.estimatedDuration, 0)

    return {
      title: content.title,
      summary: content.summary,
      fullText,
      scenes,
      backgroundMusic: content.backgroundMusic ?? undefined,
      backgroundMusicTracks: content.backgroundMusicTracks ?? undefined,
      wordCount,
      estimatedDuration,
      provider: this.getName(),
      model: this.modelName,
      usage: tokenUsage
    }
  }
}
