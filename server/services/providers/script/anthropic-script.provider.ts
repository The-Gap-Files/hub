import { z } from 'zod'
import { ChatAnthropic } from '@langchain/anthropic'
import {
  SystemMessage,
  HumanMessage,
  BaseMessage
} from '@langchain/core/messages'
import type {
  IScriptGenerator,
  ScriptGenerationRequest,
  ScriptGenerationResponse,
  ScriptScene
} from '../../../types/ai-providers'
import { buildVisualInstructionsForScript } from '../../../utils/wan-prompt-builder'

// Schema para valida√ß√£o estruturada do output (Garante JSON v√°lido e tipos corretos)
const ScriptSceneSchema = z.object({
  order: z.number().describe('A ordem sequencial da cena'),
  narration: z.string().describe('O texto que ser√° narrado pelo locutor'),
  visualDescription: z.string().describe('Descri√ß√£o t√©cnica e sensorial para o modelo de gera√ß√£o de v√≠deo (SEMPRE EM INGL√äS)'),
  audioDescription: z.string().nullable().describe('Atmosfera sonora e SFX em ingl√™s t√©cnico'),
  estimatedDuration: z.number().describe('Dura√ß√£o estimada em segundos (entre 5 e 6 segundos)')
})

const BackgroundMusicTrackSchema = z.object({
  prompt: z.string().describe('Prompt para gera√ß√£o de m√∫sica no formato Stable Audio. Inclua g√™nero, instrumentos, BPM, mood e estilo. Exemplo: "Ambient, Drone, Dark Strings, Pulsing Heartbeat Rhythm, Tension Build-Up, Mysterious, Cinematic, Atmospheric, 80 BPM"'),
  volume: z.number().describe('Volume em dB para mixagem com narra√ß√£o (-24 a -6). Prefira -12 a -10 para fundo claramente aud√≠vel; -18 fica baixo demais. Ex.: -12 m√©dio, -10 mais presente, -6 alto.'),
  startScene: z.number().describe('N√∫mero da cena onde esta track come√ßa (0 = primeira cena)'),
  endScene: z.number().nullable().describe('N√∫mero da √∫ltima cena desta track (null = at√© a √∫ltima cena do v√≠deo)')
})

const BackgroundMusicSchema = z.object({
  prompt: z.string().describe('Prompt para gera√ß√£o de m√∫sica no formato Stable Audio. Inclua g√™nero, instrumentos, BPM, mood e estilo. Exemplo: "Ambient, Drone, Dark Strings, Subtle Pads, Mysterious, Cinematic, Atmospheric, well-arranged composition, 80 BPM"'),
  volume: z.number().describe('Volume em dB para mixagem com narra√ß√£o (-24 a -6). Prefira -12 a -10 para fundo claramente aud√≠vel; -18 fica baixo demais. Ex.: -12 m√©dio, -10 mais presente, -6 alto.')
})

const ScriptResponseSchema = z.object({
  title: z.string().describe('T√≠tulo impactante para o v√≠deo'),
  summary: z.string().describe('Sinopse intrigante de 2-3 par√°grafos'),
  scenes: z.array(ScriptSceneSchema).describe('Lista de cenas que comp√µem o v√≠deo'),
  backgroundMusic: BackgroundMusicSchema.nullable().describe('M√∫sica de fundo √∫nica para TODO o v√≠deo (use apenas para v√≠deos curtos TikTok/Instagram). Use null para v√≠deos longos. Regra: "video todo"'),
  backgroundMusicTracks: z.array(BackgroundMusicTrackSchema).nullable().describe('Lista de tracks de m√∫sica de fundo por segmento de cenas (use apenas para v√≠deos longos YouTube Cinematic). Use null para v√≠deos curtos. Cada track define uma m√∫sica com prompt, volume, startScene e endScene.')
})

type ScriptResponse = z.infer<typeof ScriptResponseSchema>

export class AnthropicScriptProvider implements IScriptGenerator {
  private model: ChatAnthropic
  private modelName: string

  constructor(config: { apiKey: string; model?: string }) {
    this.modelName = config.model ?? 'claude-opus-4-6'
    this.model = new ChatAnthropic({
      anthropicApiKey: config.apiKey,
      modelName: this.modelName,
      temperature: 0.7,
      maxTokens: 64000, // Anthropic exige maxTokens expl√≠cito (64K ‚Äî limite m√°ximo do Sonnet 4; Opus aceita mais mas usamos o menor denominador)
      clientOptions: {
        timeout: 300000, // 5 minutos -- Opus com roteiros longos (YouTube Cinematic) pode demorar
        maxRetries: 3
      }
    })
  }

  getName(): string {
    return 'ANTHROPIC'
  }

  async generate(request: ScriptGenerationRequest): Promise<ScriptGenerationResponse> {
    console.log('[Anthropic Script] üé¨ Iniciando gera√ß√£o de roteiro via LangChain + Claude...')

    // Configurar o modelo para output estruturado (Zod) com includeRaw para capturar token usage
    const structuredLlm = this.model.withStructuredOutput(ScriptResponseSchema, { includeRaw: true })

    const systemPrompt = this.buildSystemPrompt(request)
    const userPrompt = this.buildUserPrompt(request)

    // Log para depura√ß√£o
    console.log('--- [DEBUG] LANGCHAIN ANTHROPIC CONFIGURATION ---')
    console.log('Model:', this.modelName)
    console.log('Target Duration:', request.targetDuration, 'seconds')
    console.log('Target WPM:', request.targetWPM)
    console.log('Ideal Scene Count:', Math.ceil(request.targetDuration / 5))
    console.log('--- [DEBUG] LANGCHAIN SYSTEM PROMPT ---')
    console.log(systemPrompt)

    // Preparar mensagens (Suporte Multimodal)
    const messages: BaseMessage[] = [
      new SystemMessage(systemPrompt)
    ]

    // Construir conte√∫do da mensagem do usu√°rio (Texto + Imagens)
    const humanContent: any[] = [
      { type: 'text', text: userPrompt }
    ]

    // Injetar imagens se dispon√≠veis (Claude Vision)
    if (request.images && request.images.length > 0) {
      console.log(`[Anthropic Script] üëÅÔ∏è Injetando ${request.images.length} imagens no contexto multimodal...`)

      request.images.forEach((img, idx) => {
        let base64Data = ''

        try {
          if (Buffer.isBuffer(img.data)) {
            base64Data = img.data.toString('base64')
          } else if (typeof img.data === 'string') {
            base64Data = img.data
          } else if (typeof img.data === 'object') {
            if ((img.data as any).type === 'Buffer' && Array.isArray((img.data as any).data)) {
              base64Data = Buffer.from((img.data as any).data).toString('base64')
            } else {
              base64Data = Buffer.from(img.data as any).toString('base64')
            }
          }
        } catch (e) {
          console.warn(`[Anthropic Script] ‚ö†Ô∏è Falha ao converter imagem ${idx}. Erro: ${e}`)
        }

        if (!base64Data) {
          console.warn(`[Anthropic Script] ‚ö†Ô∏è Imagem ${idx} ignorada: falha na extra√ß√£o de dados. Tipo: ${typeof img.data}`)
          return
        }

        // Remover prefixo data:image/...;base64, se j√° existir
        if (base64Data.includes('base64,')) {
          base64Data = base64Data.split('base64,')[1] || ''
        }

        const validMimeTypes = ['image/jpeg', 'image/png', 'image/gif', 'image/webp']
        let mimeType = (img.mimeType || 'image/jpeg').toLowerCase()
        if (mimeType === 'image/jpg') mimeType = 'image/jpeg'

        if (!validMimeTypes.includes(mimeType)) {
          console.warn(`[Anthropic Script] ‚ö†Ô∏è Imagem ${idx} ignorada: formato n√£o suportado (${mimeType}).`)
          return
        }

        // Formato de imagem para Claude via LangChain (image_url com data URI)
        humanContent.push({
          type: 'image_url',
          image_url: {
            url: `data:${mimeType};base64,${base64Data}`
          }
        })
      })
    }

    messages.push(new HumanMessage({ content: humanContent }))

    try {
      const startTime = Date.now()
      console.log('[Anthropic Script] üì§ Enviando request multimodal para LangChain + Claude...')
      console.log('[Anthropic Script] üîç Schema esperado: title, summary, scenes, backgroundMusic, backgroundMusicTracks')

      const result = await structuredLlm.invoke(messages)
      const content = result.parsed as ScriptResponse | null
      const rawMessage = result.raw as any

      const elapsed = ((Date.now() - startTime) / 1000).toFixed(2)
      console.log(`[Anthropic Script] üì• Resposta recebida em ${elapsed}s`)

      // Extrair token usage real da resposta
      const usage = rawMessage?.usage_metadata || rawMessage?.response_metadata?.usage
      const inputTokens = usage?.input_tokens ?? 0
      const outputTokens = usage?.output_tokens ?? 0
      const totalTokens = usage?.total_tokens ?? (inputTokens + outputTokens)

      console.log(`[Anthropic Script] üìä Token Usage REAL: ${inputTokens} input + ${outputTokens} output = ${totalTokens} total`)

      // Verificar se o parsing falhou (output truncado por maxTokens)
      if (!content) {
        const maxTokensHit = outputTokens >= 64000
        console.error(`[Anthropic Script] ‚ùå Parsing falhou (parsed=null). ${maxTokensHit ? 'PROV√ÅVEL CAUSA: output truncado por maxTokens (' + outputTokens + ' tokens usados).' : 'JSON inv√°lido retornado pelo modelo.'}`)
        throw new Error(`Roteiro n√£o p√¥de ser parseado. ${maxTokensHit ? 'Output excedeu o limite de tokens ‚Äî tente reduzir a dura√ß√£o do v√≠deo ou o n√∫mero de cenas.' : 'O modelo retornou JSON inv√°lido.'}`)
      }

      // Valida√ß√£o r√°pida de integridade
      console.log('[Anthropic Script] ‚úÖ Roteiro gerado com sucesso!')
      console.log('[Anthropic Script] T√≠tulo:', content.title)
      console.log('[Anthropic Script] N√∫mero de cenas:', content.scenes.length)
      console.log('[Anthropic Script] Background Music:', content.backgroundMusic ? 'Sim (video todo)' : 'N√£o')
      console.log('[Anthropic Script] Background Music Tracks:', content.backgroundMusicTracks?.length || 0, 'tracks')

      return this.parseResponse(content, request, { inputTokens, outputTokens, totalTokens })
    } catch (error) {
      console.error('[Anthropic Script] ‚ùå Erro na gera√ß√£o estruturada:', error)
      console.error('[Anthropic Script] üîç Error details:', JSON.stringify(error, null, 2))
      throw error
    }
  }

  private buildSystemPrompt(request: ScriptGenerationRequest): string {
    let styleInstructions = request.scriptStyleInstructions || 'Adote um tom documental s√©rio e investigativo.'

    let visualInstructions = ''
    if (request.visualBaseStyle) {
      visualInstructions = buildVisualInstructionsForScript({
        baseStyle: request.visualBaseStyle,
        lightingTags: request.visualLightingTags || '',
        atmosphereTags: request.visualAtmosphereTags || '',
        compositionTags: request.visualCompositionTags || '',
        generalTags: request.visualGeneralTags
      })
    } else if (request.visualStyleDescription) {
      visualInstructions = `DIRETRIZ VISUAL OBRIGAT√ìRIA: ${request.visualStyleDescription}`
    } else {
      visualInstructions = buildVisualInstructionsForScript({
        baseStyle: 'Cinematic Mystery Documentary',
        lightingTags: 'Chiaroscuro, dramatic volumetric lighting, shadows dancing',
        atmosphereTags: 'Mysterious, moody, foggy, dense atmosphere',
        compositionTags: 'Cinematic wide shots, extreme close-ups on textures',
        generalTags: '4k, highly detailed, realistic textures, grainy film look'
      })
    }

    const targetWPM = request.targetWPM || 150
    const wordsPerScene = Math.round((targetWPM / 60) * 5)
    const maxWordsHard = wordsPerScene + 2
    const wordRange = `${wordsPerScene - 1}-${wordsPerScene + 1}`

    // Determinar formato do v√≠deo para instru√ß√µes de m√∫sica
    const videoFormat = request.format || request.outputType || 'full-youtube'
    const isShortFormat = videoFormat.includes('tiktok') || videoFormat.includes('reels') || videoFormat.includes('teaser')
    const isYouTubeCinematic = videoFormat.includes('youtube') || videoFormat.includes('full')

    let musicInstructions = ''
    if (isShortFormat) {
      musicInstructions = `
---
üéµ ESTRAT√âGIA DE M√öSICA DE FUNDO (TikTok/Instagram):
- üö® REGRA: "video todo" - Use UMA m√∫sica de fundo para TODO o v√≠deo do in√≠cio ao fim
- Use o campo "backgroundMusic" com "prompt" e "volume"
- O "prompt" ser√° usado diretamente no modelo Stable Audio 2.5 para gerar a m√∫sica
- FORMATO DO PROMPT: Inclua g√™nero, sub-g√™nero, instrumentos espec√≠ficos, BPM, mood e estilo
- O "volume" √© em dB para mixagem com narra√ß√£o (-24 a -6). Prefira -12 a -10 para a m√∫sica ser claramente aud√≠vel; -18 costuma ficar baixo demais.
- Exemplo de prompt: "Ambient, Dark Drone, Subtle Synthesizer Pads, Low Strings, Mysterious, Cinematic, Atmospheric, well-arranged composition, 80 BPM"
- N√ÉO inclua volume no prompt - o volume √© um campo separado
- Exemplo completo: { prompt: "Ambient, Dark Drone, Subtle Pads, Mysterious, Cinematic, 80 BPM", volume: -12 }`
    } else if (isYouTubeCinematic) {
      musicInstructions = `
---
üéµ ESTRAT√âGIA DE M√öSICA DE FUNDO (YouTube Cinematic):
- Use a lista "backgroundMusicTracks" para definir tracks por SEGMENTO DE CENAS
- Cada track tem: "prompt" (para Stable Audio 2.5), "volume" (dB), "startScene" e "endScene"
- "startScene" = n√∫mero da cena onde a track come√ßa (0 = primeira cena)
- "endScene" = n√∫mero da √∫ltima cena desta track (null = at√© a √∫ltima cena do v√≠deo)
- O "prompt" ser√° usado diretamente no modelo Stable Audio 2.5 para gerar cada track
- FORMATO DO PROMPT: Inclua g√™nero, sub-g√™nero, instrumentos espec√≠ficos, BPM, mood e estilo
- O "volume" √© em dB para mixagem com narra√ß√£o (-24 a -6). Prefira -12 a -10 para a m√∫sica ser claramente aud√≠vel; -18 costuma ficar baixo demais.
- N√ÉO fa√ßa uma track por cena. Agrupe cenas por SEGMENTOS NARRATIVOS:
  ‚Ä¢ HOOK: Cenas iniciais ‚Äî m√∫sica de abertura impactante
  ‚Ä¢ CONTEXT: Cenas de contextualiza√ß√£o ‚Äî transi√ß√£o suave
  ‚Ä¢ RISING ACTION: Corpo principal ‚Äî intensidade crescente
  ‚Ä¢ CLIMAX: Pico narrativo ‚Äî m√°xima intensidade emocional
  ‚Ä¢ RESOLUTION + CTA: Cenas finais ‚Äî resolu√ß√£o e fechamento
- Cada segmento pode cobrir m√∫ltiplas cenas (a dura√ß√£o real ser√° calculada automaticamente)
- M√°ximo de 38 cenas por track (190s / 5s por cena = limite do modelo Stable Audio)
- Use varia√ß√µes sutis da mesma base musical por segmento
- Exemplos de tracks (para um v√≠deo de 60 cenas):
  ‚Ä¢ { prompt: "Cinematic, Impact Drums, Brass Stabs, Tension, Attention-Grabbing, Epic, 120 BPM", volume: -12, startScene: 0, endScene: 2 }
  ‚Ä¢ { prompt: "Cinematic, Building Strings, Crescendo, Tension Build-Up, Suspenseful, 100 BPM", volume: -12, startScene: 3, endScene: 8 }
  ‚Ä¢ { prompt: "Cinematic, Full Orchestra, Emotional Peak, Dramatic, Powerful, Climactic, 130 BPM", volume: -10, startScene: 9, endScene: null }`
    }

    return `Voc√™ √© um roteirista mestre em storytelling cinematogr√°fico e reten√ß√£o viral.

---
ESTILO NARRATIVO E PERSONA:
${styleInstructions}

---
DIRETRIZES T√âCNICAS (CR√çTICO):
- SINCRONIA: Cada cena DEVE durar EXATAMENTE 5 segundos de narra√ß√£o.
- DENSIDADE OBRIGAT√ìRIA: Com base na velocidade de fala (${targetWPM} WPM), cada cena DEVE conter entre ${wordsPerScene - 1} e ${maxWordsHard} palavras. A conta √©: ${targetWPM} WPM √∑ 60 √ó 5s = ${wordsPerScene} palavras ideais.
- üö® HARD LIMIT: NUNCA exceda ${maxWordsHard} palavras por cena. Cenas com mais de ${maxWordsHard} palavras ultrapassam 5 segundos e quebram a sincronia do v√≠deo.
- PROIBIDO FRASES CURTAS: Cenas com menos de ${wordsPerScene - 1} palavras geram "buracos" no √°udio. Expanda com adjetivos, detalhes sensoriais ou contexto.
- FLUIDEZ: O texto deve preencher exatamente 5 segundos de fala cont√≠nua. Nem mais, nem menos.
- SOUND DESIGN: Descreva a atmosfera sonora (SFX/Ambience) em ingl√™s t√©cnico para cada cena.
- M√öSICA DE FUNDO: Use "backgroundMusic" para v√≠deos curtos (TikTok/Instagram) ou "backgroundMusicTracks" para v√≠deos longos (YouTube). O campo "prompt" deve ser compat√≠vel com Stable Audio 2.5 (g√™nero, instrumentos, BPM, mood). O campo "volume" (dB) ser√° aplicado via FFmpeg na mixagem.
- CAMADA SENSORIAL: Nas descri√ß√µes visuais, inclua sentimentos, texturas e atmosfera.
- DIRETRIZ VISUAL: As descri√ß√µes visuais ('visualDescription') DEVEM SER SEMPRE EM INGL√äS e COMPLETAS: cada uma √© o prompt final que ser√° enviado diretamente ao modelo de imagem, sem etapa de merge. Incorpore estilo (base, ilumina√ß√£o, atmosfera, composi√ß√£o) em linguagem natural em cada cena. Seja VISUAL e CINEMATOGR√ÅFICO ‚Äî descreva cenas com impacto m√°ximo, texturas, atmosferas sombrias e composi√ß√µes dram√°ticas. Use linguagem art√≠stica intensa sem filtrar o tom.
- PERSONAGENS: Quando houver personagens recorrentes na narrativa, use SEMPRE os nomes (ou um descritor consistente, ex.: "the detective", "Maria") no visualDescription em todas as cenas em que aparecem. Isso reduz varia√ß√£o entre cenas e ajuda a manter coer√™ncia visual (ex.: "John standing by the window" em vez de "a man by the window").
- MULTIMODALIDADE: Se imagens forem fornecidas, analise-as para garantir consist√™ncia visual.
- CENAS DE ENCERRAMENTO (CTA ‚Äî OBRIGAT√ìRIO): As √∫ltimas cenas do v√≠deo (segmento CTA do plano narrativo) DEVEM incluir: (1) uma frase de gatilho para o espectador seguir o canal ‚Äî por exemplo convite para se inscrever, ativar o sininho ou acompanhar o canal, no tom do v√≠deo; (2) men√ß√£o ao canal "The Gap Files" como assinatura de encerramento. A hist√≥ria narrativa deve estar COMPLETAMENTE encerrada antes do CTA ‚Äî nunca corte uma frase no meio na √∫ltima cena de conte√∫do. Reserve as √∫ltimas 1-2 cenas exclusivamente para conclus√£o da frase/ideia e CTA.
${musicInstructions}

---
${visualInstructions}`
  }

  private buildUserPrompt(request: ScriptGenerationRequest): string {
    const targetWPM = request.targetWPM || 150
    const wordsPerScene = Math.round((targetWPM / 60) * 5)
    const minWords = wordsPerScene - 1
    const maxWords = wordsPerScene + 2
    const idealSceneCount = Math.ceil(request.targetDuration / 5)
    const maxExtraScenes = 4 // margem para concluir a hist√≥ria e CTA sem cortar frase
    const maxSceneCount = idealSceneCount + maxExtraScenes

    // Determinar formato do v√≠deo
    const videoFormat = request.format || request.outputType || 'full-youtube'
    const isShortFormat = videoFormat.includes('tiktok') || videoFormat.includes('reels') || videoFormat.includes('teaser')
    const isYouTubeCinematic = videoFormat.includes('youtube') || videoFormat.includes('full')

    let formatContext = ''
    if (isShortFormat) {
      formatContext = `\n\nüì± FORMATO DO V√çDEO: TikTok/Instagram (v√≠deo curto, 30-180s)
üö® REGRA CR√çTICA DE M√öSICA DE FUNDO:
- Use o campo "backgroundMusic" com { prompt, volume } para definir UMA m√∫sica para TODO o v√≠deo
- O "prompt" deve ser compat√≠vel com Stable Audio 2.5 (g√™nero, instrumentos, BPM, mood)
- O "volume" deve ser em dB (-24 a -6) para mixagem com narra√ß√£o`
    } else if (isYouTubeCinematic) {
      formatContext = `\n\nüé¨ FORMATO DO V√çDEO: YouTube Cinematic (v√≠deo longo, 600-3600s)
- Use a lista "backgroundMusicTracks" com tracks { prompt, volume, startScene, endScene }
- Cada track referencia CENAS (n√£o timestamps). A dura√ß√£o real ser√° calculada automaticamente.
- Agrupe cenas por segmentos narrativos (HOOK, CONTEXT, RISING ACTION, CLIMAX, RESOLUTION, CTA)
- M√∫sica pode variar por segmento narrativo, mas N√ÉO fa√ßa uma track por cena`
    }

    let baseInstruction = `Crie um roteiro em ${request.language} sobre o tema: "${request.theme}"${formatContext}`

    if (request.dossierCategory) {
      baseInstruction += `\n\nüè∑Ô∏è CLASSIFICA√á√ÉO TEM√ÅTICA: ${request.dossierCategory.toUpperCase()}`
      if (request.musicGuidance) {
        baseInstruction += `\nüéµ ORIENTA√á√ÉO MUSICAL PARA ESTA CLASSIFICA√á√ÉO: O prompt de m√∫sica DEVE seguir esta dire√ß√£o: "${request.musicGuidance}"`
        baseInstruction += `\nüíì ATMOSFERA EMOCIONAL DA TRILHA: ${request.musicMood}`
        baseInstruction += `\nUse esta orienta√ß√£o como BASE para os prompts de backgroundMusic/backgroundMusicTracks. Adapte conforme o tom do roteiro, mas mantenha a ess√™ncia da classifica√ß√£o.`
      }
      if (request.visualGuidance) {
        baseInstruction += `\n\nüñºÔ∏è ORIENTA√á√ÉO VISUAL (visualDescription): As descri√ß√µes visuais de cada cena DEVEM seguir este tom e regras: ${request.visualGuidance}`
        baseInstruction += `\nAplique esta orienta√ß√£o em TODAS as cenas. O visualDescription deve ser pronto para gera√ß√£o de imagem e alinhado ao tema do v√≠deo.`
      }
    }

    // Fontes do dossi√™ (arquitetura flat/democratizada)
    const allSources = request.sources || request.additionalSources || []
    if (allSources.length > 0) {
      baseInstruction += `\n\nüìö FONTES DO DOSSI√ä (BASE NEURAL):`
      allSources.forEach((source, index) => {
        baseInstruction += `\n[üìÑ FONTE ${index + 1}] (${source.type}): ${source.title}\n${source.content}\n---`
      })
    }

    if (request.userNotes && request.userNotes.length > 0) {
      baseInstruction += `\n\nüß† INSIGHTS E NOTAS DO AGENTE:\n${request.userNotes.join('\n- ')}`
    }

    if (request.visualReferences && request.visualReferences.length > 0) {
      baseInstruction += `\n\nüñºÔ∏è REFER√äNCIAS VISUAIS EXISTENTES (DESCRITORES):\n${request.visualReferences.join('\n- ')}`
    }

    if (request.researchData) {
      baseInstruction += `\n\nüìä DADOS ESTAT√çSTICOS/ESTRUTURADOS:\n${JSON.stringify(request.researchData, null, 2)}`
    }

    if (request.storyOutline) {
      baseInstruction += `\n\n${request.storyOutline}`
    }

    if (request.additionalContext) {
      baseInstruction += `\n\n‚ûï CONTEXTO ADICIONAL:\n${request.additionalContext}`
    }

    let guidelines = ''
    if (request.mustInclude) guidelines += `\n- DEVE INCLUIR: ${request.mustInclude}`
    if (request.mustExclude) guidelines += `\n- N√ÉO PODE CONTER: ${request.mustExclude}`

    let musicWarning = ''
    if (isShortFormat) {
      musicWarning = `\n\nüö® REGRA CR√çTICA DE M√öSICA DE FUNDO (TikTok/Instagram):
Use "backgroundMusic": { "prompt": "...", "volume": -12 } para definir UMA m√∫sica para TODO o v√≠deo (prefira volume entre -12 e -10 para ficar aud√≠vel).
O prompt deve seguir o formato Stable Audio 2.5: g√™nero, instrumentos, BPM, mood.
Defina "backgroundMusicTracks" como null.`
    } else if (isYouTubeCinematic) {
      musicWarning = `\n\nüö® REGRA CR√çTICA DE M√öSICA DE FUNDO (YouTube Cinematic):
Use "backgroundMusicTracks" com lista de tracks { prompt, volume, startScene, endScene }.
"startScene" e "endScene" s√£o N√öMEROS DE CENA (0-indexed), N√ÉO timestamps em segundos.
O prompt de cada track deve seguir o formato Stable Audio 2.5: g√™nero, instrumentos, BPM, mood.
M√°ximo de 38 cenas por track (limite do modelo). Defina "backgroundMusic" como null.`
    }

    return `${baseInstruction}

---
‚ö†Ô∏è REQUISITOS OBRIGAT√ìRIOS PARA APROVA√á√ÉO:
1. DURA√á√ÉO M√çNIMA: O v√≠deo deve ter pelo menos ${request.targetDuration} segundos (${idealSceneCount} cenas). Voc√™ PODE gerar at√© ${maxSceneCount} cenas (no m√°ximo ${maxExtraScenes} cenas extras) para concluir a hist√≥ria e o CTA sem cortar frases.
2. QUANTIDADE DE CENAS: Gere entre ${idealSceneCount} e ${maxSceneCount} cenas. Use as cenas extras APENAS para: (a) terminar a √∫ltima ideia/frase da hist√≥ria sem cortar no meio; (b) incluir o CTA completo (convite para seguir o canal + men√ß√£o The Gap Files). N√£o extrapole al√©m de ${maxSceneCount} cenas.
3. DURA√á√ÉO DA CENA: Cada cena tem slots fixos de 5 segundos.
4. CONTAGEM DE PALAVRAS: Cada narra√ß√£o DEVE ter entre ${minWords} e ${maxWords} palavras (${targetWPM} WPM √∑ 60 √ó 5s = ${wordsPerScene} palavras ideais). üö® NUNCA exceda ${maxWords} palavras - isso faz o √°udio ultrapassar 5 segundos e quebra a sincronia. NUNCA fa√ßa cenas com menos de ${minWords} palavras - isso gera sil√™ncio.
5. M√öSICA DE FUNDO: ${isShortFormat ? 'Use "backgroundMusic" { prompt, volume } para UMA m√∫sica para TODO o v√≠deo. O prompt deve ser compat√≠vel com Stable Audio 2.5.' : 'Use "backgroundMusicTracks" com tracks { prompt, volume, startTime, endTime }. O prompt de cada track deve ser compat√≠vel com Stable Audio 2.5.'}
6. Se houver imagens anexas, use-as como refer√™ncia visual prim√°ria.
${guidelines}${musicWarning}

üö® CR√çTICO: M√≠nimo ${idealSceneCount} cenas, m√°ximo ${maxSceneCount} cenas. A √∫ltima cena de conte√∫do da hist√≥ria deve terminar com frase completa. As √∫ltimas 1-2 cenas devem ser conclus√£o + CTA (seguir canal + The Gap Files).`
  }

  private parseResponse(
    content: ScriptResponse,
    request: ScriptGenerationRequest,
    tokenUsage?: { inputTokens: number; outputTokens: number; totalTokens: number }
  ): ScriptGenerationResponse {
    const scenes: ScriptScene[] = content.scenes.map((scene, index) => ({
      order: scene.order ?? index + 1,
      narration: scene.narration,
      visualDescription: scene.visualDescription,
      audioDescription: scene.audioDescription ?? undefined,
      estimatedDuration: scene.estimatedDuration ?? 5
    }))

    const fullText = scenes.map(s => s.narration).join('\n\n')
    const wordCount = fullText.split(/\s+/).length
    const estimatedDuration = scenes.reduce((acc, s) => acc + s.estimatedDuration, 0)

    return {
      title: content.title,
      summary: content.summary,
      fullText,
      scenes,
      backgroundMusic: content.backgroundMusic ?? undefined,
      backgroundMusicTracks: content.backgroundMusicTracks ?? undefined,
      wordCount,
      estimatedDuration,
      provider: this.getName(),
      model: this.modelName,
      usage: tokenUsage
    }
  }
}
